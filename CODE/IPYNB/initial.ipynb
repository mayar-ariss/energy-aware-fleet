{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import contextily as ctx\n",
    "from datetime import datetime\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import kstest\n",
    "from shapely.geometry import Polygon, Point\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect the data\n",
    "- Load the Excel file into a DataFrame and inspect it for data quality.\n",
    "- Remove rows where latitude (Lat) or longitude (Log) values are 0 in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/2022_vitals.xlsx\"\n",
    "\n",
    "# Specify the column names explicitly\n",
    "column_names = [\n",
    "    \"deviceID\", \"Timestamp\", \"Lat\", \"Log\", \"SOC_batt\", \"temp_batt\", \"volatge_batt\",\n",
    "    \"voltage_particle\", \"current_batt\", \"isCharging\", \"isCharginS\", \"isCharged\",\n",
    "    \"Temp_int\", \"Hum_int\", \"solar_current\", \"Cellular_signal_strength\",\"index\"\n",
    "]\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, header=None)  # No header initially\n",
    "\n",
    "# Process each sheet\n",
    "processed_sheets = []\n",
    "for sheet_name, sheet_data in sheets_dict.items():\n",
    "    # Ensure the number of columns matches the expected number\n",
    "    sheet_data = sheet_data.iloc[:, :len(column_names)]\n",
    "\n",
    "    # Fix misaligned rows where the first column is invalid\n",
    "    def fix_alignment(row):\n",
    "        # Convert the row to a list\n",
    "        row_list = row.tolist()\n",
    "\n",
    "        # Find the first valid `deviceID` (assumes valid `deviceID` has > 5 characters)\n",
    "        for i, value in enumerate(row_list):\n",
    "            if isinstance(value, str) and len(value) > 5:  # Valid `deviceID` found\n",
    "                # Align the row starting from the valid `deviceID`\n",
    "                aligned_row = row_list[i:i + len(column_names)]\n",
    "                # Ensure the row is padded or trimmed to match `column_names`\n",
    "                return aligned_row + [None] * (len(column_names) - len(aligned_row))\n",
    "\n",
    "        # If no valid `deviceID` is found, return a row of NaN\n",
    "        return [None] * len(column_names)\n",
    "\n",
    "    # Apply alignment fix to all rows\n",
    "    sheet_data = sheet_data.apply(fix_alignment, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # Assign column names\n",
    "    sheet_data.columns = column_names\n",
    "\n",
    "    # Drop rows where 'deviceID' is still invalid or starts with \"deviceID\"\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'].notna()]\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'] != \"deviceID\"]  # Remove rows starting with \"deviceID\"\n",
    "\n",
    "    # Append processed sheet\n",
    "    processed_sheets.append(sheet_data)\n",
    "\n",
    "# Concatenate all sheets into one DataFrame\n",
    "df = pd.concat(processed_sheets, ignore_index=True)\n",
    "\n",
    "# Drop rows where Lat or Log is 0\n",
    "df = df[(df['Lat'] != 0) & (df['Log'] != 0)]\n",
    "\n",
    "# Correct the indexing column to start at 1 and increment sequentially\n",
    "df.reset_index(drop=True, inplace=True)  # Reset pandas index\n",
    "df['index'] = df.index + 1  # Create a 1-based index\n",
    "\n",
    "# Save the cleaned data back to Excel (optional)\n",
    "output_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/2022_vitals_cleaned.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print the cleaned data preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Timestamps and Create a Spatial Grid\n",
    "- Convert the Timestamp column from Unix format to human-readable datetime.\n",
    "- Group the GPS data into a spatial grid for coverage analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Ensure 'Lat' and 'Log' are numeric\n",
    "df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "df['Log'] = pd.to_numeric(df['Log'], errors='coerce')\n",
    "\n",
    "# Spatial Resolution: 120m grid\n",
    "grid=120\n",
    "lat_resolution = grid / 111320  \n",
    "df['Lat_Grid'] = (df['Lat'] // lat_resolution) * lat_resolution\n",
    "\n",
    "# Longitude resolution depends on latitude\n",
    "df['Lon_Resolution'] = grid / (111320 * np.cos(np.radians(df['Lat'])))\n",
    "df['Log_Grid'] = (df['Log'] // df['Lon_Resolution']) * df['Lon_Resolution']\n",
    "\n",
    "# Temporal Resolution: 10s intervals\n",
    "# temporal_resolution = '10s'  \n",
    "# df['Time_Bin'] = df['Timestamp'].dt.floor(temporal_resolution)\n",
    "\n",
    "# Drop auxiliary column\n",
    "df = df.drop(columns=['Lon_Resolution'])\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Detect continuous stationary periods\n",
    "df['Prev_Lat_Grid'] = df.groupby('deviceID')['Lat_Grid'].shift(1)\n",
    "df['Prev_Log_Grid'] = df.groupby('deviceID')['Log_Grid'].shift(1)\n",
    "df['Prev_Timestamp'] = df.groupby('deviceID')['Timestamp'].shift(1)\n",
    "\n",
    "# Step 3: Identify whether the taxi has stayed in the same grid\n",
    "df['Same_Grid'] = (df['Lat_Grid'] == df['Prev_Lat_Grid']) & (df['Log_Grid'] == df['Prev_Log_Grid'])\n",
    "\n",
    "# Step 4: Compute time spent in the grid continuously\n",
    "df['Time_Diff'] = (df['Timestamp'] - df['Prev_Timestamp']).dt.total_seconds()\n",
    "\n",
    "# Step 5: Assign a group ID that resets when the taxi leaves a grid\n",
    "df['Group'] = (~df['Same_Grid']).cumsum()\n",
    "\n",
    "# Step 6: Compute total time spent in each visit to the grid\n",
    "df['Cumulative_Time'] = df.groupby(['deviceID', 'Lat_Grid', 'Log_Grid', 'Group'])['Time_Diff'].cumsum()\n",
    "\n",
    "# Step 7: Remove vehicles that stayed continuously in the same grid for more than 3hours (10800 sec)\n",
    "df = df[~(df['Cumulative_Time'] >= 10800)]\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['Prev_Lat_Grid', 'Prev_Log_Grid', 'Prev_Timestamp', 'Same_Grid', 'Time_Diff', 'Group', 'Cumulative_Time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative sampling maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define the bounds from the DataFrame\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Approximate latitude resolution\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid of polygons\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Increment the count for each grid square containing points\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n",
    "\n",
    "# Apply Fractional Power Scaling to 'Count'\n",
    "gamma = 0.3  # Adjust between 0.2 - 0.5 for better visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "\n",
    "# Plot the grid using the scaled count with a jet colormap\n",
    "grid_gdf.plot(\n",
    "    column='Scaled_Count',\n",
    "    cmap='jet',\n",
    "    legend=True,\n",
    "    alpha=0.4,\n",
    "    ax=ax,\n",
    "    # edgecolor='gray'\n",
    ")\n",
    "\n",
    "# Add a basemap\n",
    "ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Adjust plot aesthetics\n",
    "ax.set_title('Heatmap of Measurements Across Stockholm (120x120m Grid)', fontsize=12)\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 10 most frequently sensed grid cells\n",
    "top_10_cells = grid_gdf.nlargest(10, 'Count')\n",
    "\n",
    "# Filter the data for points within these top 10 grid cells\n",
    "df_top_cells = df_gdf[df_gdf.geometry.apply(lambda point: any(top_10_cells.contains(point)))].copy()  # Make a copy\n",
    "\n",
    "# Compute the time differences for each device in each grid cell\n",
    "df_top_cells['Time_Diff'] = df_top_cells.groupby(['Lat_Grid', 'Log_Grid', 'deviceID'])['Timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "# Calculate the mean sampling frequency (1 / mean time difference) per grid cell\n",
    "sampling_frequency = df_top_cells.groupby(['Lat_Grid', 'Log_Grid'])['Time_Diff'].mean().dropna().apply(lambda x: 1 / x)\n",
    "\n",
    "top_10_frequencies = sampling_frequency.nlargest(10)\n",
    "# Convert to DataFrame for better visualization\n",
    "top_10_frequencies_df = top_10_frequencies.reset_index()\n",
    "top_10_frequencies_df.columns = ['Lat_Grid', 'Log_Grid', 'Sampling_Frequency (Hz)']\n",
    "top_10_frequencies_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded with the necessary data\n",
    "# Identify unique days where at least one device's SOC_batt dropped below 50%\n",
    "df['Date'] = df['Timestamp'].dt.date  # Extract the date\n",
    "days_with_depletion = df[df['SOC_batt'] < 50]['Date'].nunique()\n",
    "\n",
    "# Display the number of days with a battery drop below 50%\n",
    "days_with_depletion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "depletion_threshold = 50\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create Transitions (from one grid cell to the next)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop last row per vehicle (no next transition available)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build the Markov Transition Matrix\n",
    "transition_counts = (\n",
    "    df_transitions.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities = transition_counts.div(transition_counts.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# Step 7: Define a Function to Predict the Next Grid Cell\n",
    "def predict_next_grid(current_grid, transition_matrix):\n",
    "    if current_grid in transition_matrix.index:\n",
    "        return transition_matrix.loc[current_grid].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 8: Validate Predictions Using Post-Depletion Data\n",
    "df_post_depletion['Predicted_Grid_Cell'] = df_post_depletion['Grid_Cell'].apply(\n",
    "    lambda x: predict_next_grid(x, transition_probabilities)\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Prediction Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Step 10: Save the Transition Matrix and Post-Depletion Validation\n",
    "output_path_transition_matrix = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix.xlsx\"\n",
    "transition_probabilities.to_excel(output_path_transition_matrix)\n",
    "\n",
    "output_path_post_depletion = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation.xlsx\"\n",
    "df_post_depletion.to_excel(output_path_post_depletion, index=False)\n",
    "\n",
    "# Step 11: Analyze and Print the Top 10 Most Likely Transitions\n",
    "most_likely_transitions = (\n",
    "    transition_probabilities.stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'Probability', 'level_0': 'From_Grid', 'level_1': 'To_Grid'})\n",
    "    .sort_values(by='Probability', ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top 10 Most Likely Transitions:\")\n",
    "print(most_likely_transitions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Post-Depletion Data to GeoDataFrame for Visualization\n",
    "df_post_depletion = gpd.GeoDataFrame(df_post_depletion, \n",
    "                                     geometry=gpd.points_from_xy(df_post_depletion['Log_Grid'], df_post_depletion['Lat_Grid']), \n",
    "                                     crs=\"EPSG:4326\")\n",
    "\n",
    "# Convert Predicted Grid Cells to Points\n",
    "predicted_points = df_post_depletion[['Predicted_Grid_Cell']].dropna().copy()\n",
    "predicted_points['Lat_Pred'] = predicted_points['Predicted_Grid_Cell'].apply(lambda x: grid_gdf.loc[x, 'geometry'].centroid.y if x in grid_gdf.index else None)\n",
    "predicted_points['Lon_Pred'] = predicted_points['Predicted_Grid_Cell'].apply(lambda x: grid_gdf.loc[x, 'geometry'].centroid.x if x in grid_gdf.index else None)\n",
    "predicted_points = predicted_points.dropna()\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_predicted = gpd.GeoDataFrame(predicted_points, \n",
    "                                 geometry=gpd.points_from_xy(predicted_points['Lon_Pred'], predicted_points['Lat_Pred']), \n",
    "                                 crs=\"EPSG:4326\")\n",
    "\n",
    "# Plot the actual and predicted trajectories\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Plot Grid\n",
    "grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.4)\n",
    "\n",
    "# Plot Actual Trajectory (Post-Depletion)\n",
    "df_post_depletion.plot(ax=ax, color='red', markersize=20, label=\"Actual Trajectory\", alpha=0.7)\n",
    "\n",
    "# Plot Predicted Trajectory\n",
    "gdf_predicted.plot(ax=ax, color='blue', markersize=20, label=\"Predicted Trajectory\", alpha=0.7)\n",
    "\n",
    "# Add Basemap\n",
    "ctx.add_basemap(ax, crs=df_post_depletion.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Actual vs Predicted Trajectory (Post-Depletion)\", fontsize=14)\n",
    "ax.legend()\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day and generate a plot\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current day\n",
    "    df_day = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_pre_depletion_day = df_pre_depletion[df_pre_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert actual, predicted, and pre-depletion data into GeoDataFrames\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'\n",
    "\n",
    "    predicted_grid_cells = df_day['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'\n",
    "\n",
    "    pre_depletion_grid_cells = df_pre_depletion_day['Grid_Cell'].unique()\n",
    "    gdf_pre_depletion = grid_gdf.loc[grid_gdf.index.isin(pre_depletion_grid_cells)].copy()\n",
    "    gdf_pre_depletion['Color'] = 'black'\n",
    "\n",
    "    # Skip plotting if all GeoDataFrames are empty for the day\n",
    "    if gdf_actual.empty and gdf_predicted.empty and gdf_pre_depletion.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot for the current day\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black) if not empty\n",
    "    if not gdf_pre_depletion.empty:\n",
    "        gdf_pre_depletion.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Trajectory (Red) if not empty\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory\")\n",
    "\n",
    "    # Plot Predicted Trajectory (Blue) if not empty\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Actual vs Predicted Trajectory (Post-Depletion) - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CURRENT \n",
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"No Constraint\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define line styles for each threshold\n",
    "line_styles = {\n",
    "    \"No Constraint\": \"-\",\n",
    "    \"3 sec\": \"--\",\n",
    "    \"12 sec\": \":\"\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['red', 'blue', 'green', 'black', 'purple']\n",
    "device_ids = set()\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            marker='o',\n",
    "            markersize=3,\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    " \n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "\n",
    "                d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                diff = max(d_diff_prev, d_diff)\n",
    "                stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                stored_energy[grid_key] += diff\n",
    "                \n",
    "                if diff != 0: \n",
    "                    df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                else:\n",
    "                    df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                d_diff_prev=diff\n",
    "                print(f\"[DEBUG] OFF: Accumulated {diff:.2f}% for grid {grid_key}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "            if grid_key not in stored_energy:\n",
    "                if i > 0:\n",
    "                    #Carry forward the stored energy from the last known row\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                elif i == 0:\n",
    "                    stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                    df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "            \n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby('Date')[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"No Constraint\"] = df_temp.groupby('Date')['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n",
    "# Plot SOC depletion for different thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "for label in soc_depletion_df.columns:\n",
    "    plt.plot(soc_depletion_df.index, soc_depletion_df[label], marker='o', linestyle='-', label=label)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Different Time Constraints (With Energy Storage)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp = df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp per grid cell and stored energy per device\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}  \n",
    "    last_device_row = {}  # To track the last row where each device appeared\n",
    "\n",
    "    # Previous date\n",
    "    prev_date = None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        device_id = row['deviceID']\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        device_key = (device_id, row['Lat_Grid'], row['Log_Grid'])  # Unique per device-grid\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "\n",
    "        # Initialize inter-row differences when OFF\n",
    "        d_diff_prev = 0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy.clear()  # Reset stored energy for all devices\n",
    "            df_temp.loc[df_temp['Date'] == current_date, f'Energy_Saved_{label}'] = 0  \n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        # Reset stored energy if SOC is at maximum for this device\n",
    "        if df_temp.loc[i, 'SOC_batt'] > 99:\n",
    "            stored_energy[device_key] = 0\n",
    "            df_temp.at[i, f'Energy_Saved_{label}'] = 0\n",
    "\n",
    "        # **Grid-based sensing decision (shared across devices)**\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # **Device-specific energy storage**\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "                d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt'])  # Current inter-row difference\n",
    "                diff = max(d_diff_prev, d_diff)\n",
    "\n",
    "                if device_key not in stored_energy:\n",
    "                    stored_energy[device_key] = 0  # Ensure initialized storage\n",
    "\n",
    "                # Find the last preceding row for this device\n",
    "                if device_id in last_device_row:\n",
    "                    prev_idx = last_device_row[device_id]  # The last row where this device appeared\n",
    "                    stored_energy[device_key] = df_temp.at[prev_idx, f'Energy_Saved_{label}']\n",
    "                    stored_energy[device_key] += diff  # Accumulate for this device only\n",
    "\n",
    "                    # Store the energy saved for this specific device\n",
    "                    if diff != 0: \n",
    "                        df_temp.at[i, f'Energy_Saved_{label}'] = stored_energy[device_key]\n",
    "                    else:\n",
    "                        df_temp.at[i, f'Energy_Saved_{label}'] = df_temp.at[prev_idx, f'Energy_Saved_{label}']\n",
    "                else:\n",
    "                    df_temp.at[i, f'Energy_Saved_{label}'] = 0  # If no previous row, start at 0\n",
    "\n",
    "                d_diff_prev = diff\n",
    "                print(f\"[DEBUG] OFF: Accumulated {diff:.2f}% for device {device_id} at grid {grid_key}. Total stored: {stored_energy[device_key]:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            # **Grid-based sensing decision**\n",
    "            last_sensed_time[grid_key] = current_time  # Same grid key shared across devices\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev = 0\n",
    "\n",
    "            # Ensure stored_energy is initialized per device without overwriting previous values\n",
    "            if device_key not in stored_energy:\n",
    "                if device_id in last_device_row:\n",
    "                    prev_idx = last_device_row[device_id]  # Get last row for this device\n",
    "                    # Carry forward the stored energy from the last known row for this device\n",
    "                    stored_energy[device_key] = df_temp.at[prev_idx, f'Energy_Saved_{label}']\n",
    "                else:\n",
    "                    stored_energy[device_key] = 0  # No previous entry, initialize to 0\n",
    "\n",
    "                df_temp.at[i, f'Energy_Saved_{label}'] = stored_energy[device_key]\n",
    "\n",
    "        # Update last row for this device\n",
    "        last_device_row[device_id] = i  \n",
    "\n",
    "    # Compute new SOC_batt with savings per device\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)  # Cap SOC at 100%\n",
    "\n",
    "    # Compute SOC depletion for this threshold per device\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean().unstack()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints per device\n",
    "soc_depletion_results[\"No Constraint\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean().unstack()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.concat(soc_depletion_results, axis=1)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n",
    "# Define line styles for each threshold\n",
    "line_styles = {\n",
    "    \"No Constraint\": \":\",\n",
    "    \"3 sec\": \"--\",\n",
    "    \"12 sec\": \"-\"\n",
    "}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, df_temp in soc_depletion_results.items():\n",
    "\n",
    "    for device_id in df_temp.columns:  # Iterate over devices\n",
    "        plt.plot(df_temp.index, df_temp[device_id], linestyle=line_styles[label], marker='o', label=f\"Device {device_id} - {label}\")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count of times the sensor was turned OFF for each constraint scenario\n",
    "off_counts = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "    df_copy = df.copy()  # Work on a copy of the dataset\n",
    "    df_copy['Sensor_ON'] = True  # Default: Sensor is ON\n",
    "\n",
    "    # Track last sensed timestamp per grid cell\n",
    "    last_sensed_time = {}\n",
    "    off_count = 0\n",
    "\n",
    "    for i, row in df_copy.iterrows():\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_copy.at[i, 'Sensor_ON'] = False\n",
    "            off_count += 1\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "\n",
    "    off_counts[label] = off_count\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "off_counts_df = pd.DataFrame.from_dict(off_counts, orient='index', columns=['Sensor OFF Count'])\n",
    "\n",
    "# Display results\n",
    "off_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary to store debug results\n",
    "debug_metrics = {}\n",
    "\n",
    "# 1️⃣ Count how many times the sensor is ON vs OFF per day\n",
    "sensor_on_off_daily = df.groupby('Date')['Sensor_ON'].value_counts().unstack().fillna(0)\n",
    "sensor_on_off_daily.columns = ['Sensor_OFF_Count', 'Sensor_ON_Count']\n",
    "debug_metrics[\"Sensor ON/OFF Count Per Day\"] = sensor_on_off_daily\n",
    "\n",
    "# 2️⃣ Check SOC statistics per day (flatten multi-index)\n",
    "soc_stats_daily = df.groupby('Date')[['SOC_batt', 'SOC_Without_Constraint']].agg(['mean', 'min', 'max'])\n",
    "soc_stats_daily.columns = ['_'.join(col) for col in soc_stats_daily.columns]  # Flatten columns\n",
    "debug_metrics[\"SOC Statistics Per Day\"] = soc_stats_daily\n",
    "\n",
    "# 3️⃣ Compare SOC drop per day (difference between first and last SOC reading)\n",
    "daily_soc_change = df.groupby('Date').agg(\n",
    "    First_SOC=('SOC_batt', 'first'),\n",
    "    Last_SOC=('SOC_batt', 'last'),\n",
    "    First_SOC_Without_Constraint=('SOC_Without_Constraint', 'first'),\n",
    "    Last_SOC_Without_Constraint=('SOC_Without_Constraint', 'last')\n",
    ")\n",
    "daily_soc_change['SOC_Change'] = daily_soc_change['Last_SOC'] - daily_soc_change['First_SOC']\n",
    "daily_soc_change['SOC_Change_Without_Constraint'] = daily_soc_change['Last_SOC_Without_Constraint'] - daily_soc_change['First_SOC_Without_Constraint']\n",
    "debug_metrics[\"Daily SOC Change\"] = daily_soc_change[['SOC_Change', 'SOC_Change_Without_Constraint']]\n",
    "\n",
    "# 4️⃣ Check if Charging Frequency is Affecting SOC per day\n",
    "charging_events_per_day = df.groupby('Date')['isCharging'].sum().to_frame()  # Ensure it's a DataFrame\n",
    "debug_metrics[\"Charging Events Per Day\"] = charging_events_per_day\n",
    "\n",
    "# 5️⃣ Check how frequently the sensor is turning ON/OFF (total transitions per day)\n",
    "df['Sensor_Transition'] = df['Sensor_ON'].diff().abs()\n",
    "sensor_transitions_per_day = df.groupby('Date')['Sensor_Transition'].sum().to_frame()  # Ensure it's a DataFrame\n",
    "debug_metrics[\"Sensor ON/OFF Transitions Per Day\"] = sensor_transitions_per_day\n",
    "\n",
    "# Convert debug metrics to a single DataFrame for easy viewing\n",
    "debug_df = pd.concat(debug_metrics.values(), axis=1, keys=debug_metrics.keys())\n",
    "debug_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store debug results\n",
    "debug_metrics = {}\n",
    "\n",
    "# 1️⃣ Check if Charging Frequency Increases When the 10-Minute Rule is Applied\n",
    "charging_events_per_day = df.groupby('Date')['isCharging'].sum()\n",
    "debug_metrics[\"Charging Events Per Day\"] = charging_events_per_day\n",
    "\n",
    "# 2️⃣ Compare SOC Drop During Charging vs. Not Charging\n",
    "soc_during_charging = df.groupby(['Date', 'isCharging'])['SOC_batt'].mean().unstack()\n",
    "debug_metrics[\"SOC During Charging vs Not Charging\"] = soc_during_charging\n",
    "\n",
    "# 3️⃣ Compare Average Energy Consumption Per Active Hour\n",
    "avg_batt_current_on = df[df['Sensor_ON']].groupby('Date')['current_batt'].mean()\n",
    "debug_metrics[\"Avg Battery Current When Sensor is ON\"] = avg_batt_current_on\n",
    "\n",
    "# 4️⃣ Check if Solar Current is Higher Without Constraints\n",
    "solar_current_comparison = df.groupby(['Date', 'Sensor_ON'])['solar_current'].mean().unstack()\n",
    "debug_metrics[\"Solar Charging When Sensor is ON vs OFF\"] = solar_current_comparison\n",
    "\n",
    "# 5️⃣ Compare the Number of Unique Grid Cells Covered\n",
    "unique_grid_cells_per_day = df.groupby('Date')['Lat_Grid'].nunique()\n",
    "debug_metrics[\"Unique Grid Cells Covered Per Day\"] = unique_grid_cells_per_day\n",
    "\n",
    "# Convert debug metrics to a single DataFrame for easy viewing\n",
    "debug_df = pd.concat(debug_metrics.values(), axis=1, keys=debug_metrics.keys())\n",
    "\n",
    "# Display the debug DataFrame\n",
    "debug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define battery capacity in mAh\n",
    "BATTERY_CAPACITY = 10000  \n",
    "\n",
    "# Define the time interval threshold (10 minutes in seconds)\n",
    "TIME_THRESHOLD = 600  \n",
    "\n",
    "# Convert the Timestamp column to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Step 1: Sort data by device and timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Step 2: Identify sensor activity based on 10-minute rule\n",
    "df['Sensor_ON'] = True  # Default: Sensor is ON\n",
    "\n",
    "# Track the last sensing timestamp per grid cell\n",
    "last_sensed_time = {}\n",
    "last_SOC = {}\n",
    "\n",
    "off_count = 0  # Counter for how many times the sensor turns OFF\n",
    "\n",
    "print(\"\\n=== DEBUG LOG: SENSOR ON/OFF STATUS ===\")\n",
    "for i, row in df.iterrows():\n",
    "    grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "    current_time = row['Timestamp']\n",
    "\n",
    "    # If the grid was sensed recently (within 10 minutes), turn OFF the sensor\n",
    "    if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "        df.at[i, 'Sensor_ON'] = False\n",
    "        off_count += 1\n",
    "        print(f\"[OFF] {row['Timestamp']} | Grid: {grid_key} | Device: {row['deviceID']} | SOC: {row['SOC_batt']:.2f}%\")\n",
    "    else:\n",
    "        # Update last sensed time when the sensor turns ON\n",
    "        last_sensed_time[grid_key] = current_time\n",
    "        last_SOC[grid_key] = row['SOC_batt']\n",
    "        print(f\"[ON]  {row['Timestamp']} | Grid: {grid_key} | Device: {row['deviceID']} | SOC: {row['SOC_batt']:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal times sensor was turned OFF due to 10-min rule: {off_count}\\n\")\n",
    "\n",
    "# Step 3: Calculate SOC changes only when the sensor is ON\n",
    "df['SOC_Change'] = 0.0  # Default no change\n",
    "\n",
    "print(\"\\n=== DEBUG LOG: SOC CHANGES ===\")\n",
    "for i in range(1, len(df)):\n",
    "    if df.at[i, 'Sensor_ON']:  # Process only if sensor is ON\n",
    "        dt = (df.at[i, 'Timestamp'] - df.at[i-1, 'Timestamp']).total_seconds() / 3600  # Time difference in hours\n",
    "        I_batt = df.at[i, 'current_batt']  # Current in mA\n",
    "        SOC_change_mAh = -1 * I_batt * dt  # Calculate change in mAh\n",
    "        SOC_change_percentage = (SOC_change_mAh / BATTERY_CAPACITY) * 100  # Convert to percentage\n",
    "\n",
    "        # Apply SOC change\n",
    "        df.at[i, 'SOC_Change'] = SOC_change_percentage\n",
    "        df.at[i, 'SOC_batt'] = max(0, min(100, df.at[i-1, 'SOC_batt'] + SOC_change_percentage))  # Keep SOC in range\n",
    "\n",
    "        print(f\"[SOC UPDATE] {df.at[i, 'Timestamp']} | Device: {df.at[i, 'deviceID']} | ΔSOC: {SOC_change_percentage:.2f}% | New SOC: {df.at[i, 'SOC_batt']:.2f}%\")\n",
    "\n",
    "# Step 4: Compute the daily increased coverage due to energy conservation\n",
    "df['Date'] = df['Timestamp'].dt.date  # Extract date\n",
    "\n",
    "# Count the number of unique grid cells sensed each day\n",
    "daily_coverage = df[df['Sensor_ON']].groupby('Date')['Lat_Grid'].nunique().reset_index()\n",
    "daily_coverage.columns = ['Date', 'Unique_Grid_Cells_Sensed']\n",
    "\n",
    "# Compute SOC depletion without the 10-minute constraint (baseline scenario)\n",
    "df['SOC_Without_Constraint'] = df['SOC_batt'].copy()\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    dt = (df.at[i, 'Timestamp'] - df.at[i-1, 'Timestamp']).total_seconds() / 3600  # Time difference in hours\n",
    "    I_batt = df.at[i, 'current_batt']  # Current in mA\n",
    "    SOC_change_mAh = -1 * I_batt * dt  # Calculate change in mAh\n",
    "    SOC_change_percentage = (SOC_change_mAh / BATTERY_CAPACITY) * 100  # Convert to percentage\n",
    "\n",
    "    # Apply SOC change without the 10-minute constraint\n",
    "    df.at[i, 'SOC_Without_Constraint'] = max(0, min(100, df.at[i-1, 'SOC_Without_Constraint'] + SOC_change_percentage))\n",
    "\n",
    "# Group by date to calculate mean SOC per day\n",
    "soc_depletion_comparison = df.groupby('Date')[['SOC_batt', 'SOC_Without_Constraint']].mean()\n",
    "\n",
    "# Plot SOC depletion comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(soc_depletion_comparison.index, soc_depletion_comparison['SOC_batt'], marker='o', label='With 10-min Constraint')\n",
    "plt.plot(soc_depletion_comparison.index, soc_depletion_comparison['SOC_Without_Constraint'], marker='s', linestyle='dashed', label='Without 10-min Constraint')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison (With vs Without 10-Min Constraint)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum sensing interval (in seconds)\n",
    "min_sensing_interval = 600  # 10 minutes\n",
    "\n",
    "# Make a fresh copy of df to avoid persistent memory issues in Jupyter Notebook\n",
    "df_all = df.copy()\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df_all = df_all.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Initialize tracking variables\n",
    "df_all['Last_Sensed_Timestamp'] = pd.NaT  \n",
    "df_all['Turn_Off_Sensor'] = False  # Default is ON\n",
    "\n",
    "# Dictionary to store the last valid ON timestamp per grid cell\n",
    "last_sensed_time = {}\n",
    "\n",
    "# DEBUGGING: Create a list to track ON/OFF transitions\n",
    "debug_logs = []\n",
    "\n",
    "for index, row in df_all.iterrows():\n",
    "    # ✅ Standardize grid_key to avoid floating point mismatches\n",
    "    grid_key = (round(row['Lat_Grid'], 6), round(row['Log_Grid'], 6))  # Rounding avoids precision mismatches\n",
    "    current_time = row['Timestamp']\n",
    "\n",
    "    if grid_key in last_sensed_time:\n",
    "        time_diff = (current_time - last_sensed_time[grid_key]).total_seconds()\n",
    "\n",
    "        # ✅ Enforce 10-minute OFF period\n",
    "        if time_diff < min_sensing_interval:\n",
    "            df_all.at[index, 'Turn_Off_Sensor'] = True  # Sensor stays OFF\n",
    "            debug_logs.append(f\"🔴 OFF: {current_time} at {grid_key} (time_diff={time_diff:.2f}s)\")\n",
    "        else:\n",
    "            # ✅ If 10 minutes have passed, allow new measurement\n",
    "            df_all.at[index, 'Turn_Off_Sensor'] = False  # Sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time  # ✅ Update timestamp\n",
    "            df_all.at[index, 'Last_Sensed_Timestamp'] = current_time\n",
    "            debug_logs.append(f\"🟢 ON: {current_time} at {grid_key} (time_diff={time_diff:.2f}s)\")\n",
    "    else:\n",
    "        # ✅ First-time sensing this grid cell, allow sensing\n",
    "        df_all.at[index, 'Turn_Off_Sensor'] = False  # Sensor is ON\n",
    "        last_sensed_time[grid_key] = current_time\n",
    "        df_all.at[index, 'Last_Sensed_Timestamp'] = current_time\n",
    "        debug_logs.append(f\"🟢 ON (First Time): {current_time} at {grid_key}\")\n",
    "\n",
    "# ✅ Print debug logs to verify ON/OFF behavior\n",
    "for log in debug_logs[:20]:  # Print first 20 logs to check behavior\n",
    "    print(log)\n",
    "\n",
    "# Step 3: Ensure battery remains constant while the sensor is OFF\n",
    "df_all['SOC_batt_with_constraint'] = df_all['SOC_batt']  # Initialize new SOC values\n",
    "\n",
    "# Carry forward last known SOC for OFF events\n",
    "df_all['Prev_SOC'] = df_all.groupby('deviceID')['SOC_batt_with_constraint'].shift(1)\n",
    "df_all.loc[df_all['Turn_Off_Sensor'], 'SOC_batt_with_constraint'] = df_all.loc[df_all['Turn_Off_Sensor'], 'Prev_SOC']\n",
    "\n",
    "# Step 4: Predict battery depletion dynamically when the sensor turns back ON\n",
    "df_all['Δt_hours'] = df_all.groupby('deviceID')['Timestamp'].diff().dt.total_seconds() / 3600  # Convert time to hours\n",
    "df_all['ΔSOC'] = -1 * df_all['current_batt'] * df_all['Δt_hours'] / 10000 * 100  # Battery depletion as %\n",
    "\n",
    "# Apply depletion only when the sensor is ON\n",
    "df_all.loc[~df_all['Turn_Off_Sensor'], 'SOC_batt_with_constraint'] = df_all.loc[~df_all['Turn_Off_Sensor'], 'Prev_SOC'] + df_all.loc[~df_all['Turn_Off_Sensor'], 'ΔSOC']\n",
    "\n",
    "# Step 5: Ensure all OFF states inherit the **same** SOC from the first OFF event\n",
    "df_all['SOC_batt_with_constraint'] = df_all.groupby(['deviceID', 'Lat_Grid', 'Log_Grid'])['SOC_batt_with_constraint'].fillna(method='ffill')\n",
    "\n",
    "# Step 6: Clip values to ensure SOC stays between 0% - 100%\n",
    "df_all['SOC_batt_with_constraint'] = df_all['SOC_batt_with_constraint'].fillna(df_all['SOC_batt'])\n",
    "df_all['SOC_batt_with_constraint'] = df_all['SOC_batt_with_constraint'].clip(0, 100)\n",
    "\n",
    "# Remove helper columns\n",
    "df_all = df_all.drop(columns=['Prev_SOC', 'Δt_hours', 'ΔSOC'])\n",
    "\n",
    "# Step 7: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df_all, geometry=gpd.points_from_xy(df_all['Log'], df_all['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure 'Grid_Cell' column exists\n",
    "def find_grid_cell(row, grid_gdf):\n",
    "    match = grid_gdf.contains(row.geometry)\n",
    "    return match.idxmax() if match.any() else None\n",
    "\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 8: Compute spatio-temporal coverage\n",
    "df_active_sensing = df_gdf[~df_gdf['Turn_Off_Sensor']].copy()\n",
    "\n",
    "# Count the number of unique grid cells sensed per day\n",
    "df_active_sensing['Date'] = df_active_sensing['Timestamp'].dt.date\n",
    "daily_spatio_temporal_coverage = df_active_sensing.groupby('Date')['Grid_Cell'].nunique()\n",
    "\n",
    "# Step 9: Aggregate daily SOC values\n",
    "df_gdf['Date'] = df_gdf['Timestamp'].dt.date\n",
    "soc_daily_no_constraint = df_gdf.groupby('Date')['SOC_batt'].mean()\n",
    "soc_daily_with_constraint = df_gdf.groupby('Date')['SOC_batt_with_constraint'].mean()\n",
    "\n",
    "# Step 10: Plot SOC depletion comparison (With vs Without the 10-minute constraint)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(soc_daily_no_constraint.index, soc_daily_no_constraint, label=\"Always ON (No Constraint)\", color=\"red\", linestyle=\"--\", marker=\"o\")\n",
    "ax.plot(soc_daily_with_constraint.index, soc_daily_with_constraint, label=\"With 10-min Rule\", color=\"green\", linestyle=\"-\", marker=\"o\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Battery SOC Depletion: Always ON vs 10-Minute Sensing Constraint\", fontsize=14)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(\"Average SOC (%)\", fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"Updated spatio-temporal coverage and energy savings calculated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all[['Timestamp', 'Lat_Grid', 'Log_Grid', 'Turn_Off_Sensor', 'Last_Sensed_Timestamp']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute total battery consumption before and after applying the 10-minute rule\n",
    "df_gdf['SOC_batt_before'] = df_gdf['SOC_batt']  # Before applying 10min rule\n",
    "df_gdf['SOC_batt_after'] = df_gdf['SOC_batt_new']  # After applying 10min rule\n",
    "\n",
    "# Aggregate by day\n",
    "df_gdf['Date'] = df_gdf['Timestamp'].dt.date\n",
    "energy_usage_before = df_gdf.groupby('Date')['SOC_batt_before'].mean()\n",
    "energy_usage_after = df_gdf.groupby('Date')['SOC_batt_after'].mean()\n",
    "\n",
    "# Step 2: Plot the energy savings\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(energy_usage_before.index, energy_usage_before, label=\"Before 10-min Rule\", color=\"red\", linestyle=\"--\", marker=\"o\")\n",
    "ax.plot(energy_usage_after.index, energy_usage_after, label=\"After 10-min Rule\", color=\"green\", linestyle=\"-\", marker=\"o\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Average Battery SOC Before and After 10-Minute Sensing Interval\", fontsize=14)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(\"Average SOC (%)\", fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define different sensing intervals to test (in seconds)\n",
    "sensing_intervals = [600, 900, 3600, 28800, 86400]  # 10min, 15min, 1h, 8h, 24h\n",
    "\n",
    "# Store results in a dictionary\n",
    "energy_savings = {}\n",
    "\n",
    "for interval in sensing_intervals:\n",
    "    # Identify oversampled grid cells (visited within less than the given interval)\n",
    "    df_gdf[f'Prev_Timestamp_{interval}'] = df_gdf.groupby(['deviceID', 'Lat_Grid', 'Log_Grid'])['Timestamp'].shift(1)\n",
    "    df_gdf[f'Time_Diff_{interval}'] = (df_gdf['Timestamp'] - df_gdf[f'Prev_Timestamp_{interval}']).dt.total_seconds()\n",
    "    df_gdf[f'Turn_Off_Sensor_{interval}'] = df_gdf[f'Time_Diff_{interval}'] < interval\n",
    "\n",
    "    # Compute SOC change for active sensing periods\n",
    "    df_gdf[f'Δt_hours_{interval}'] = df_gdf[f'Time_Diff_{interval}'] / 3600  # Convert time difference to hours\n",
    "    df_gdf[f'ΔSOC_mAh_{interval}'] = -1 * df_gdf['current_batt'] * df_gdf[f'Δt_hours_{interval}']  # Compute SOC change in mAh\n",
    "\n",
    "    # Apply SOC update formula only when sensors are ON\n",
    "    df_gdf[f'ΔSOC_percent_{interval}'] = (df_gdf[f'ΔSOC_mAh_{interval}'] / C_batt) * 100\n",
    "    df_gdf[f'SOC_batt_{interval}'] = df_gdf['SOC_batt']  # Initialize new SOC values\n",
    "    df_gdf.loc[~df_gdf[f'Turn_Off_Sensor_{interval}'], f'SOC_batt_{interval}'] += df_gdf.loc[~df_gdf[f'Turn_Off_Sensor_{interval}'], f'ΔSOC_percent_{interval}']\n",
    "    \n",
    "    # Ensure SOC stays within valid range (0% - 100%)\n",
    "    df_gdf[f'SOC_batt_{interval}'] = df_gdf[f'SOC_batt_{interval}'].clip(0, 100)\n",
    "\n",
    "    # Aggregate by day\n",
    "    energy_savings[interval] = df_gdf.groupby(df_gdf['Timestamp'].dt.date)[f'SOC_batt_{interval}'].mean()\n",
    "\n",
    "# Step 2: Plot the energy savings across different intervals\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot before 10-min rule\n",
    "ax.plot(energy_usage_before.index, energy_usage_before, label=\"Before Any Rule\", color=\"red\", linestyle=\"--\", marker=\"o\")\n",
    "\n",
    "# Plot results for different sensing intervals\n",
    "colors = ['green', 'blue', 'purple', 'orange', 'brown']\n",
    "interval_labels = [\"10 min\", \"15 min\", \"1 hr\", \"8 hrs\", \"24 hrs\"]\n",
    "\n",
    "for i, interval in enumerate(sensing_intervals):\n",
    "    ax.plot(energy_savings[interval].index, energy_savings[interval], label=f\"After {interval_labels[i]} Rule\", color=colors[i], linestyle=\"-\", marker=\"o\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Average Battery SOC Before and After Different Sensing Intervals\", fontsize=14)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(\"Average SOC (%)\", fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day for visualization\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current depleted day\n",
    "    df_day_pre = df_pre_coverage[df_pre_coverage['Date'] == day]\n",
    "    df_day_new = df_new_coverage_only[df_new_coverage_only['Date'] == day]\n",
    "    df_day_actual = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_day_predicted = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert to GeoDataFrames\n",
    "    gdf_pre = grid_gdf.loc[grid_gdf.index.isin(df_day_pre['Grid_Cell'])].copy()\n",
    "    gdf_pre['Color'] = 'black'  # Pre-depletion trajectory\n",
    "\n",
    "    gdf_new = grid_gdf.loc[grid_gdf.index.isin(df_day_new['Grid_Cell'])].copy()\n",
    "    gdf_new['Color'] = 'green'  # Newly sensed due to 10min rule\n",
    "\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day_actual['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'  # Actual trajectory after depletion\n",
    "\n",
    "    predicted_grid_cells = df_day_predicted['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'  # Predicted trajectory\n",
    "\n",
    "    # Skip if no relevant data for the day\n",
    "    if gdf_pre.empty and gdf_new.empty and gdf_actual.empty and gdf_predicted.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells (Background)\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black)\n",
    "    if not gdf_pre.empty:\n",
    "        gdf_pre.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Post-Depletion Trajectory (Red)\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory (Post-Depletion)\")\n",
    "\n",
    "    # Plot Predicted Post-Depletion Trajectory (Blue)\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Plot Newly Sensed Cells Due to 10-Minute Rule (Green)\n",
    "    if not gdf_new.empty:\n",
    "        gdf_new.plot(ax=ax, color='green', alpha=0.5, edgecolor='green', label=\"Newly Sensed Cells (10-min Interval)\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Trajectory Visualization with 10-Minute Sensing Constraint - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the bounds from the data\n",
    "min_lat, max_lat = df['Lat'].min(), df['Lat'].max()\n",
    "min_lon, max_lon = df['Log'].min(), df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Convert meters to latitude degrees\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid covering the dataset area\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the data points\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Assign each measurement to a grid square\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n",
    "\n",
    "# Apply Fractional Power Scaling\n",
    "gamma = 0.3  # Adjust for visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Normalize values for color mapping\n",
    "norm = Normalize(vmin=grid_gdf['Scaled_Count'].min(), vmax=grid_gdf['Scaled_Count'].max())\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "# Convert scaled values to hex colors\n",
    "grid_gdf['Color'] = grid_gdf['Scaled_Count'].apply(lambda x: to_hex(cmap(norm(x))))\n",
    "\n",
    "# Create Folium map centered on Stockholm\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Function to color the grid based on scaled counts\n",
    "def style_function(feature):\n",
    "    color = feature['properties']['Color']  # Get precomputed color\n",
    "    return {\n",
    "        'fillColor': color,\n",
    "        'color': 'black',\n",
    "        'weight': 0.1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    "\n",
    "# Add grid layer to Folium\n",
    "folium.GeoJson(\n",
    "    grid_gdf,\n",
    "    name=\"Measurement Grid\",\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['Count'], aliases=[\"Measurements:\"])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Analysis\n",
    "#### Check Data Distribution\n",
    "- Before plotting, inspect the distribution of the Count column to confirm the skew. If the Count values have a large range (e.g., some counts are much higher than others), you can apply a logarithmic scale to the color mapping. This makes smaller variations more distinguishable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by spatial grid and count occurrences\n",
    "coverage = df.groupby(['Lat_Grid', 'Log_Grid']).size().reset_index(name='Count')\n",
    "\n",
    "# Count the frequency of each unique coverage count\n",
    "coverage_freq = coverage['Count'].value_counts().reset_index()\n",
    "coverage_freq.columns = ['Coverage Count', 'Frequency']\n",
    "\n",
    "# Sort in descending order\n",
    "coverage_freq = coverage_freq.sort_values(by='Coverage Count', ascending=False)\n",
    "\n",
    "# Find the maximum coverage count\n",
    "max_count = coverage['Count'].max()\n",
    "\n",
    "sns.histplot(coverage['Count'], bins=max_count, kde=True, color='blue')\n",
    "plt.title('Distribution of Coverage Counts')\n",
    "plt.xlabel('Coverage Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# # Define Zipf-Mandelbrot function\n",
    "# def zipf_mandelbrot_func(r, s, q, C):\n",
    "#     return C / (r + np.abs(q)) ** s  # Ensure q is positive\n",
    "\n",
    "# # Define resolutions to test\n",
    "# spatial_resolutions = [0.00001, 0.0001, 0.001, 0.01]\n",
    "# temporal_resolutions = ['10S', '30S', '1T', '5T']\n",
    "\n",
    "# # Store results\n",
    "# results = []\n",
    "\n",
    "# for spatial_resolution in spatial_resolutions:\n",
    "#     for temporal_resolution in temporal_resolutions:\n",
    "#         # Create spatial grid\n",
    "#         df['Lat_Grid'] = (df['Lat'] // spatial_resolution) * spatial_resolution\n",
    "#         df['Log_Grid'] = (df['Log'] // spatial_resolution) * spatial_resolution\n",
    "        \n",
    "#         # Create temporal bins\n",
    "#         df['Time_Bin'] = df['Timestamp'].dt.floor(temporal_resolution)\n",
    "\n",
    "#         # Group by spatial grid and count occurrences\n",
    "#         coverage = df.groupby(['Lat_Grid', 'Log_Grid']).size().reset_index(name='Count')\n",
    "\n",
    "#         # Sort data in Zipfian order\n",
    "#         sorted_counts = np.sort(coverage['Count'])[::-1]  # Descending order\n",
    "#         ranks = np.arange(1, len(sorted_counts) + 1)  # Rank numbers\n",
    "\n",
    "#         # Fit Zipf-Mandelbrot\n",
    "#         try:\n",
    "#             params, _ = curve_fit(zipf_mandelbrot_func, ranks, sorted_counts, \n",
    "#                                   p0=[1, 1, max(sorted_counts)], \n",
    "#                                   bounds=([0.5, 0.0001, 0], [3, 10, np.inf]))\n",
    "\n",
    "#             s_fit, q_fit, C_fit = params\n",
    "#             expected_values = zipf_mandelbrot_func(ranks, s_fit, q_fit, C_fit)\n",
    "            \n",
    "#             # Compute residuals\n",
    "#             residuals = sorted_counts - expected_values\n",
    "#             std_residuals = np.std(residuals)\n",
    "            \n",
    "#             # Perform KS test\n",
    "#             ks_stat, p_value = kstest(sorted_counts, zipf_mandelbrot_func, args=(s_fit, q_fit, C_fit))\n",
    "\n",
    "#             # Compute AIC (Akaike Information Criterion)\n",
    "#             AIC = -2 * np.log(p_value) + 2 * 3  # 3 parameters: s, q, C\n",
    "\n",
    "#             # Store results\n",
    "#             results.append({\n",
    "#                 'Spatial_Resolution': spatial_resolution,\n",
    "#                 'Temporal_Resolution': temporal_resolution,\n",
    "#                 'KS_Statistic': ks_stat,\n",
    "#                 'p_value': p_value,\n",
    "#                 'Std_Residuals': std_residuals,\n",
    "#                 'AIC': AIC\n",
    "#             })\n",
    "\n",
    "#         except RuntimeError:\n",
    "#             print(f\"Fit failed for Spatial={spatial_resolution}, Temporal={temporal_resolution}\")\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # Select the best resolution (min AIC, high p-value, low KS statistic)\n",
    "# best_result = results_df.sort_values(by=['AIC', 'KS_Statistic'], ascending=[True, True]).iloc[0]\n",
    "# print(\"Best Resolution Parameters:\")\n",
    "# print(best_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort coverage counts in descending order (ranked frequencies)\n",
    "sorted_counts = np.sort(coverage['Count'])[::-1]  # Descending order\n",
    "ranks = np.arange(1, len(sorted_counts) + 1)  # Rank numbers\n",
    "\n",
    "# Define Zipf-Mandelbrot function: f(r) = C / (r + q)^s\n",
    "def zipf_mandelbrot_func(r, s, q, C):\n",
    "    return C / (r + np.abs(q)) ** s  # Ensure q is positive\n",
    "\n",
    "# Fit Zipf-Mandelbrot with constraints to avoid numerical issues\n",
    "params, _ = curve_fit(zipf_mandelbrot_func, ranks, sorted_counts, p0=[1, 1, max(sorted_counts)], bounds=([0.5, 0.0001, 0], [3, 10, np.inf]))\n",
    "s_fit, q_fit, C_fit = params\n",
    "\n",
    "# Compute expected values from the fitted Zipf-Mandelbrot model\n",
    "expected_values = zipf_mandelbrot_func(ranks, s_fit, q_fit, C_fit)\n",
    "\n",
    "# Compute residuals (Observed - Expected)\n",
    "residuals = sorted_counts - expected_values\n",
    "relative_residuals = residuals / expected_values  # Normalize residuals\n",
    "\n",
    "# Plot Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(ranks, residuals, alpha=0.6, color=\"red\", label=\"Residuals (Observed - Expected)\")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", alpha=0.6)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Residual (Observed - Expected)\", fontsize=12)\n",
    "plt.title(\"Residuals from Zipf-Mandelbrot Fit\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for outliers (e.g., 1.2x expected value)\n",
    "tolerance_factor = 3\n",
    "\n",
    "# Identify outliers (values too far from expected Zipfian behavior)\n",
    "outlier_mask = (sorted_counts > expected_values * tolerance_factor) | (sorted_counts < expected_values / tolerance_factor)\n",
    "\n",
    "# Count the number of removed points\n",
    "num_outliers = outlier_mask.sum()\n",
    "print(f\"Number of detected outliers: {num_outliers}\")\n",
    "\n",
    "# Remove outliers from dataset\n",
    "filtered_counts = sorted_counts[~outlier_mask]\n",
    "filtered_ranks = ranks[~outlier_mask]\n",
    "\n",
    "# Exclude extreme values (top 5% and bottom 5%)\n",
    "lower_bound_c = int(0.1 * len(filtered_counts))\n",
    "upper_bound_c = int(0.9 * len(filtered_counts))\n",
    "filtered_counts=filtered_counts[lower_bound_c:upper_bound_c]\n",
    "lower_bound_r = int(0.1 * len(filtered_ranks))\n",
    "upper_bound_r = int(0.9 * len(filtered_ranks))\n",
    "filtered_ranks = filtered_ranks[lower_bound_r:upper_bound_r]\n",
    "\n",
    "# Re-Fit Zipf-Mandelbrot with filtered data\n",
    "params_filtered, _ = curve_fit(\n",
    "    zipf_mandelbrot_func, \n",
    "    filtered_ranks, \n",
    "    filtered_counts, \n",
    "    p0=[1, 1, max(filtered_counts)], \n",
    "    bounds=([0.5, 0.0001, 0], [3, 10, np.inf])\n",
    ")\n",
    "\n",
    "# Extract new parameters\n",
    "s_fit_filtered, q_fit_filtered, C_fit_filtered = params_filtered\n",
    "\n",
    "# Compute expected values with new parameters\n",
    "expected_values_filtered = zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    "\n",
    "# Plot cleaned data vs. new Zipf-Mandelbrot fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_ranks, filtered_counts, label=\"Filtered Data (No Outliers)\", alpha=0.6, color=\"blue\")\n",
    "plt.plot(filtered_ranks, expected_values_filtered, 'r-', linewidth=2, label=\"Re-Fitted Zipf-Mandelbrot Model\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Coverage Count\", fontsize=12)\n",
    "plt.title(\"Zipf-Mandelbrot Fit After Outlier Removal & Re-Fitting\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude extreme values (top 5% and bottom 5%)\n",
    "lower_bound = int(0.1 * len(filtered_counts))\n",
    "upper_bound = int(0.9 * len(filtered_counts))\n",
    "\n",
    "ks_stat_truncated, p_value_truncated = kstest(\n",
    "    filtered_counts[lower_bound:upper_bound], \n",
    "    zipf_mandelbrot_func, \n",
    "    args=(s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    ")\n",
    "ks_stat_after, p_value_after = kstest(filtered_counts, zipf_mandelbrot_func, args=(s_fit_filtered, q_fit_filtered, C_fit_filtered))\n",
    "\n",
    "\n",
    "print(f\"Truncated KS Test - Statistic: {ks_stat_truncated:.4f}, p-value: {p_value_truncated:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estimated Zipf-Mandelbrot Exponent (s): {s_fit:.4f}\")\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute log-likelihood\n",
    "log_likelihood = np.sum(norm.logpdf(filtered_counts, loc=zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered), scale=np.std(filtered_counts)))\n",
    "AIC_fixed = -2 * log_likelihood + 2 * 3  # 3 parameters: s, q, C\n",
    "\n",
    "print(f\"Fixed AIC: {AIC_fixed:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pure Zipf function: f(r) = C / r^s\n",
    "def zipf_func(r, s, C):\n",
    "    return C / r ** s\n",
    "\n",
    "# Fit pure Zipf\n",
    "params_zipf, _ = curve_fit(zipf_func, filtered_ranks, filtered_counts, \n",
    "                           p0=[1, max(filtered_counts)], \n",
    "                           bounds=([0.5, 0], [3, np.inf]))\n",
    "\n",
    "s_zipf, C_zipf = params_zipf\n",
    "expected_values_zipf = zipf_func(filtered_ranks, s_zipf, C_zipf)\n",
    "\n",
    "# Compute Log-Likelihood and AIC for pure Zipf\n",
    "log_likelihood_zipf = np.sum(norm.logpdf(\n",
    "    filtered_counts, \n",
    "    loc=expected_values_zipf, \n",
    "    scale=np.std(filtered_counts)\n",
    "))\n",
    "AIC_zipf = -2 * log_likelihood_zipf + 2 * 2  # 2 parameters: s, C\n",
    "\n",
    "print(f\"Pure Zipf Log-Likelihood: {log_likelihood_zipf:.4f}\")\n",
    "print(f\"Pure Zipf AIC: {AIC_zipf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10% (frequent ranks)\n",
    "top_residuals = filtered_counts[:int(0.1 * len(filtered_counts))] - expected_values_filtered[:int(0.1 * len(filtered_counts))]\n",
    "\n",
    "# Bottom 10% (rare ranks)\n",
    "bottom_residuals = filtered_counts[-int(0.1 * len(filtered_counts)):] - expected_values_filtered[-int(0.1 * len(filtered_counts)):]\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(filtered_ranks[:int(0.1 * len(filtered_ranks))], top_residuals, color='red', alpha=0.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Top 10% Residuals\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(filtered_ranks[-int(0.1 * len(filtered_ranks)):], bottom_residuals, color='blue', alpha=0.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Bottom 10% Residuals\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on middle 80% of ranks\n",
    "lower_bound = int(0.1 * len(filtered_counts))\n",
    "upper_bound = int(0.9 * len(filtered_counts))\n",
    "\n",
    "ks_stat_truncated, p_value_truncated = kstest(\n",
    "    filtered_counts[lower_bound:upper_bound], \n",
    "    zipf_mandelbrot_func, \n",
    "    args=(s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    ")\n",
    "\n",
    "print(f\"Truncated KS Test - Statistic: {ks_stat_truncated:.4f}, p-value: {p_value_truncated:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_values_filtered = zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    "residuals_filtered = filtered_counts - expected_values_filtered\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_ranks, residuals_filtered, alpha=0.6, color=\"red\", label=\"Residuals (Observed - Expected)\")\n",
    "\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", alpha=0.6)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Residual (Observed - Expected)\", fontsize=12)\n",
    "plt.title(\"Residuals After Outlier Removal\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "- Analyze sensor coverage by aggregating the spatial grid.\n",
    "- Visualize coverage heatmaps. To better visualize the data, apply logarithmic scaling to the color values. This will compress the range of large values and expand the smaller values for more differentiation in color. \n",
    "- We apply Fractional Power Scaling: Highlights smaller values significantly, making subtle differences more visible. We Raise the log-transformed values to a fractional power $ \\log(x+1)^{0.5} $. This amplifies small differences while keeping the general scale.\n",
    "##### Note:\n",
    "- If \\( x = 0 \\), the standard `np.log(x)` would result in an error because the logarithm of 0 is undefined. \n",
    "`np.log1p(x)` handles this safely by adding \\( 1 \\) to the input before computing the logarithm, ensuring it works for non-negative numbers, including \\( 0 \\).\n",
    "- The square root further compresses the range of the values.  \n",
    "It emphasizes smaller differences by reducing the impact of large values. For example:  \n",
    "$\\log(x+1)^{0.5} $ grows slower than $\\log(x+1)$ as $\\ x $ increases.\n",
    "\n",
    "**This transformation is particularly useful for skewed data, for `Count` values, where:**\n",
    "\n",
    "- Most data points are small.\n",
    "- A few extreme values (outliers) dominate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Apply logarithmic scaling\n",
    "log_scaled_values = np.log1p(coverage['Count'])**0.5 \n",
    "\n",
    "# Apply logarithmic scaling to color values\n",
    "sc = plt.scatter(\n",
    "    coverage['Log_Grid'], \n",
    "    coverage['Lat_Grid'], \n",
    "    c=log_scaled_values,  \n",
    "    cmap='jet', \n",
    "    s=30, \n",
    "    edgecolor='k', \n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add a color bar with the original scale in the label\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('√(Log(Coverage Count + 1))', fontsize=12)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add labels and title with improved font sizes\n",
    "plt.xlabel('Longitude Grid', fontsize=14)\n",
    "plt.ylabel('Latitude Grid', fontsize=14)\n",
    "plt.title('Coverage Heatmap with Logarithmic Scale', fontsize=16)\n",
    "\n",
    "# Add grid lines for reference\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Improve tick sizes for better readability\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Temporal Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time difference in seconds between consecutive rows\n",
    "df['Delta_t'] = df['Timestamp'].diff().dt.total_seconds()  \n",
    "\n",
    "# Define an expected interval in seconds (e.g., 60 seconds)\n",
    "expected_interval = 60\n",
    "\n",
    "# Count the total number of occurrences of measurements\n",
    "tot_count = df['Delta_t'].count()\n",
    "print(f\"Number of values: {tot_count}\")\n",
    "\n",
    "# Count the number of occurrences of low frequency measurements\n",
    "highf_count = (df['Delta_t'] > expected_interval).sum()\n",
    "print(f\"Number of values higher than 60sec: {highf_count}\")\n",
    "\n",
    "# Count the number of occurrences of 0.0\n",
    "zero_count = (df['Timestamp'].diff().dt.total_seconds() == 0.0).sum()\n",
    "print(f\"Number of values equal to 0.0sec: {zero_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy and Coverage Model Preparation\n",
    "- Create columns to represent:\n",
    "\n",
    "    - Whether a street segment is already covered.\n",
    "    - Battery state changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering out Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time difference in seconds between consecutive rows\n",
    "df['Delta_t'] = df['Timestamp'].diff().dt.total_seconds()  \n",
    "\n",
    "# Define a threshold for acceptable intervals (e.g., 60 seconds)\n",
    "acceptable_threshold = 60   # in seconds\n",
    "\n",
    "# Filter out rows with large Delta_t\n",
    "df = df[df['Delta_t'] <= acceptable_threshold]\n",
    "\n",
    "# Drop rows with Delta_t equal to zero\n",
    "df = df[df['Delta_t'] > 0]\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The battery capacity is 10,000 mAh, so SOC_Change (calculated from current and time) must be converted into a percentage of the total capacity before being added to `SOC_batt`.\n",
    "\n",
    "### SOC Update Formula:\n",
    "$SOC_{new} = SOC_{old} + \\frac{\\Delta SOC_{mAh}}{C_{batt}} \\times 100$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\Delta SOC_{mAh} = -1 \\times I_{batt} \\times \\Delta t$ (mAh change based on current and time)\n",
    "\n",
    "Where:\n",
    "- $SOC_{old}$ is $SOC_{batt}$\n",
    "- $\\ I_{batt} $: Net current (`current_batt`) in mA (positive for consumption, negative for storage).\n",
    "- $\\ \\Delta t $: Time difference in hours between consecutive rows.\n",
    "- $\\ C_{batt} $: Battery capacity in mAh (10,000 mAh).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Battery capacity in mAh\n",
    "battery_capacity = 10000\n",
    "\n",
    "# Calculate time difference in hours between consecutive rows\n",
    "df['Delta_t'] = df['Delta_t'] / 3600  # Time difference in hours\n",
    "\n",
    "# Calculate SOC change (in %) using the corrected formula\n",
    "df['SOC_Change'] = (-1 * df['current_batt'] * df['Delta_t'] / battery_capacity) * 100\n",
    "\n",
    "# Set SOC_Change to 0 when SOC is saturated\n",
    "df.loc[df['SOC_batt'] >= 90, 'SOC_Change'] = 0\n",
    "\n",
    "# Ensure SOC values are capped between 0 and 100\n",
    "df['SOC_batt'] = df['SOC_batt'] + df['SOC_Change']\n",
    "df['SOC_batt'] = df['SOC_batt'].clip(lower=0, upper=100)\n",
    "\n",
    "# Assume that is >90%, charging is stopped\n",
    "df.loc[df['SOC_batt'] >= 90, 'SOC_Change'] = 0\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(df[['Timestamp', 'Lat', 'Log', 'SOC_batt', 'SOC_Change', 'Delta_t', 'current_batt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SOC over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Timestamp'], df['SOC_batt'], label='State of Charge (SOC)', color='blue', linewidth=1.5)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('SOC (%)', fontsize=14)\n",
    "plt.title('Battery State of Charge Over Time', fontsize=16)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where SOC is increasing or decreasing\n",
    "increasing = df[df['SOC_Change'] > 0]\n",
    "decreasing = df[df['SOC_Change'] < 0]\n",
    "\n",
    "print(f\"Number of times SOC increases: {len(increasing)}\")\n",
    "print(f\"Number of times SOC decreases: {len(decreasing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark covered segments (spatio-temporal condition)\n",
    "df['Is_Covered'] = df.duplicated(subset=['Lat_Grid', 'Log_Grid', 'Time_Bin'], keep='first')\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_aware_switch(row):\n",
    "    if row['Is_Covered'] and row['SOC_batt'] > 20:\n",
    "        return 'OFF'\n",
    "    elif not row['Is_Covered'] and row['SOC_batt'] > 10:\n",
    "        return 'ON'\n",
    "    else:\n",
    "        return 'IDLE'\n",
    "\n",
    "df['Sensor_State'] = df.apply(energy_aware_switch, axis=1)\n",
    "\n",
    "df['Cumulative_Spatial_Coverage'] = (~df['Is_Covered']).cumsum()\n",
    "temporal_coverage = df.groupby(['Lat_Grid', 'Log_Grid', 'Time_Bin']).size().reset_index(name='Frequency')\n",
    "\n",
    "high_coverage_cells = temporal_coverage[temporal_coverage['Frequency'] > 2]\n",
    "high_coverage_cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cumulative coverage and energy tracking\n",
    "df['Cumulative_Coverage'] = 0\n",
    "df['Average_SOC'] = 0\n",
    "\n",
    "# Initialize variables\n",
    "cumulative_coverage = set()  # To store unique covered grid cells\n",
    "soc_list = []  # To store SOC levels\n",
    "\n",
    "# Simulate over the data\n",
    "for idx, row in df.iterrows():\n",
    "    # Update cumulative coverage if sensor is ON\n",
    "    if row['Sensor_State'] == 'ON':\n",
    "        cumulative_coverage.add((row['Lat_Grid'], row['Log_Grid']))\n",
    "\n",
    "    # Update SOC tracking\n",
    "    soc_list.append(row['SOC_batt'])\n",
    "\n",
    "    # Update DataFrame\n",
    "    df.at[idx, 'Cumulative_Coverage'] = len(cumulative_coverage)\n",
    "    df.at[idx, 'Average_SOC'] = sum(soc_list) / len(soc_list)\n",
    "\n",
    "# Preview results\n",
    "print(df[['Timestamp', 'Cumulative_Coverage', 'Average_SOC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative coverage over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Timestamp'], df['Cumulative_Coverage'], label='Cumulative Coverage', color='b')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Cumulative Coverage', fontsize=14)\n",
    "plt.title('Cumulative Coverage Over Time', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot average SOC over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Timestamp'], df['Average_SOC'], label='Average SOC', color='g')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Average SOC (%)', fontsize=14)\n",
    "plt.title('Average SOC Over Time', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: All sensors ON\n",
    "df['Baseline_Coverage'] = 0\n",
    "df['Baseline_SOC'] = 0.0\n",
    "\n",
    "# Initialize variables\n",
    "baseline_coverage = set()\n",
    "baseline_soc_list = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Assume sensors are always ON\n",
    "    baseline_coverage.add((row['Lat_Grid'], row['Log_Grid']))\n",
    "    baseline_soc_list.append(row['SOC_batt'])\n",
    "\n",
    "    # Update DataFrame\n",
    "    df.at[idx, 'Baseline_Coverage'] = len(baseline_coverage)\n",
    "    df.at[idx, 'Baseline_SOC'] = sum(baseline_soc_list) / len(baseline_soc_list)\n",
    "\n",
    "# Compare cumulative coverage and SOC\n",
    "print(df[['Timestamp', 'Cumulative_Coverage', 'Baseline_Coverage', 'Average_SOC', 'Baseline_SOC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cumulative coverage\n",
    "energy_aware_coverage = df['Cumulative_Coverage'].iloc[-1]\n",
    "baseline_coverage = df['Baseline_Coverage'].iloc[-1]\n",
    "\n",
    "# Average SOC\n",
    "energy_aware_avg_soc = df['Average_SOC'].mean()\n",
    "baseline_avg_soc = df['Baseline_SOC'].mean()\n",
    "\n",
    "# Improvement metrics\n",
    "coverage_improvement = (energy_aware_coverage - baseline_coverage) / baseline_coverage * 100\n",
    "soc_savings = (baseline_avg_soc - energy_aware_avg_soc) / baseline_avg_soc * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Energy-Aware Total Coverage: {energy_aware_coverage}\")\n",
    "print(f\"Baseline Total Coverage: {baseline_coverage}\")\n",
    "print(f\"Coverage Improvement: {coverage_improvement:.2f}%\")\n",
    "print(f\"Energy Savings: {soc_savings:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scl_rf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
