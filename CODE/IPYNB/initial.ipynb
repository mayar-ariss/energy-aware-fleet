{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries Overview\n",
    "\n",
    "This cell imports essential Python libraries for geospatial analysis, data visualization, and statistical modeling:\n",
    "\n",
    "- **Data Handling & Computation**: `numpy` (`np`), `pandas` (`pd`), `collections.defaultdict`\n",
    "- **Geospatial Analysis**: `geopandas` (`gpd`), `shapely.geometry` (`Polygon`, `Point`), `contextily` (`ctx`), `folium`\n",
    "- **Visualization**: `matplotlib.pyplot` (`plt`), `seaborn` (`sns`), `matplotlib.colors` (`Normalize`, `to_hex`)\n",
    "- **Statistical Analysis**: `scipy.stats`, `scipy.optimize.curve_fit`, `kstest`\n",
    "- **Date & Time**: `datetime.datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import contextily as ctx\n",
    "from datetime import datetime\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import kstest\n",
    "from shapely.geometry import Polygon, Point\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Loads and process multi-sheet Excel data\n",
    "\n",
    "1. **File Loading**: Reads all sheets from `2022_vitals.xlsx` without headers.\n",
    "2. **Column Naming**: Assigns predefined column names for consistency.\n",
    "3. **Data Alignment**: \n",
    "   - Fixes misaligned rows by detecting valid `deviceID`.\n",
    "   - Ensures all rows have the correct number of columns.\n",
    "4. **Filtering**:\n",
    "   - Removes invalid or duplicate header rows.\n",
    "   - Drops rows with zero values for latitude (`Lat`) and longitude (`Log`).\n",
    "5. **Indexing**: Resets the index and assigns a sequential 1-based index.\n",
    "6. **Output**: Saves the cleaned data to `2022_vitals_cleaned.xlsx` and previews it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/2022_vitals.xlsx\"\n",
    "\n",
    "# Specify the column names explicitly\n",
    "column_names = [\n",
    "    \"deviceID\", \"Timestamp\", \"Lat\", \"Log\", \"SOC_batt\", \"temp_batt\", \"volatge_batt\",\n",
    "    \"voltage_particle\", \"current_batt\", \"isCharging\", \"isCharginS\", \"isCharged\",\n",
    "    \"Temp_int\", \"Hum_int\", \"solar_current\", \"Cellular_signal_strength\",\"index\"\n",
    "]\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, header=None)  # No header initially\n",
    "\n",
    "# Process each sheet\n",
    "processed_sheets = []\n",
    "for sheet_name, sheet_data in sheets_dict.items():\n",
    "    # Ensure the number of columns matches the expected number\n",
    "    sheet_data = sheet_data.iloc[:, :len(column_names)]\n",
    "\n",
    "    # Fix misaligned rows where the first column is invalid\n",
    "    def fix_alignment(row):\n",
    "        # Convert the row to a list\n",
    "        row_list = row.tolist()\n",
    "\n",
    "        # Find the first valid `deviceID` (assumes valid `deviceID` has > 5 characters)\n",
    "        for i, value in enumerate(row_list):\n",
    "            if isinstance(value, str) and len(value) > 5:  # Valid `deviceID` found\n",
    "                # Align the row starting from the valid `deviceID`\n",
    "                aligned_row = row_list[i:i + len(column_names)]\n",
    "                # Ensure the row is padded or trimmed to match `column_names`\n",
    "                return aligned_row + [None] * (len(column_names) - len(aligned_row))\n",
    "\n",
    "        # If no valid `deviceID` is found, return a row of NaN\n",
    "        return [None] * len(column_names)\n",
    "\n",
    "    # Apply alignment fix to all rows\n",
    "    sheet_data = sheet_data.apply(fix_alignment, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # Assign column names\n",
    "    sheet_data.columns = column_names\n",
    "\n",
    "    # Drop rows where 'deviceID' is still invalid or starts with \"deviceID\"\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'].notna()]\n",
    "    sheet_data = sheet_data[sheet_data['deviceID'] != \"deviceID\"]  # Remove rows starting with \"deviceID\"\n",
    "\n",
    "    # Append processed sheet\n",
    "    processed_sheets.append(sheet_data)\n",
    "\n",
    "# Concatenate all sheets into one DataFrame\n",
    "df = pd.concat(processed_sheets, ignore_index=True)\n",
    "\n",
    "# Drop rows where Lat or Log is 0\n",
    "df = df[(df['Lat'] != 0) & (df['Log'] != 0)]\n",
    "\n",
    "# Correct the indexing column to start at 1 and increment sequentially\n",
    "df.reset_index(drop=True, inplace=True)  # Reset pandas index\n",
    "df['index'] = df.index + 1  # Create a 1-based index\n",
    "\n",
    "# Save the cleaned data back to Excel (optional)\n",
    "output_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/2022_vitals_cleaned.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Print the cleaned data preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spatiotemporal Binning and Stationary Period Detection**\n",
    "\n",
    "This enables **spatial binning**, **stationary period detection**, and **temporal filtering** for robust movement analysis.\n",
    "\n",
    "## **1. Timestamp Conversion**\n",
    "The Unix timestamp $ T_i $ is converted into a standard datetime format:\n",
    "\n",
    "$$\n",
    "T_i^{\\text{datetime}} = T_i^{\\text{unix}} \\times \\frac{1}{86400} + \\text{epoch}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ T_i^{\\text{unix}} $ is the raw Unix timestamp in **seconds**,\n",
    "- $ 86400 $ seconds = **1 day**,\n",
    "- **Epoch** is the reference starting time (January 1, 1970).\n",
    "\n",
    "## **2. Ensuring Numeric Latitude and Longitude**\n",
    "We enforce that latitude ($ \\text{Lat} $) and longitude ($ \\text{Log} $) are real-valued:\n",
    "\n",
    "$$\n",
    "\\text{Lat}, \\text{Log} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Non-numeric values are coerced to **NaN**.\n",
    "\n",
    "## **3. Discretization into a 120m Grid**\n",
    "### **3.1 Latitude Grid Resolution**\n",
    "Since the **Earth's meridional circumference** is approximately **40,030 km**, the degree-to-meter conversion near the equator is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111,320 \\text{ meters}\n",
    "$$\n",
    "\n",
    "Thus, the spatial resolution of a **120m grid** in latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Lat} = \\frac{120}{111320}\n",
    "$$\n",
    "\n",
    "The **grid-aligned latitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Lat\\_Grid} = \\left\\lfloor \\frac{\\text{Lat}}{\\Delta \\text{Lat}} \\right\\rfloor \\times \\Delta \\text{Lat}\n",
    "$$\n",
    "\n",
    "### **3.2 Longitude Grid Resolution**\n",
    "Unlike latitude, **longitude spacing** varies with latitude due to Earthâ€™s curvature. The **longitude degree-to-meter conversion** is:\n",
    "\n",
    "$$\n",
    "1^\\circ \\approx 111320 \\times \\cos(\\text{Lat})\n",
    "$$\n",
    "\n",
    "Thus, the **longitude resolution** at a given latitude is:\n",
    "\n",
    "$$\n",
    "\\Delta \\text{Log} = \\frac{120}{111320 \\cos(\\text{Lat})}\n",
    "$$\n",
    "\n",
    "The **grid-aligned longitude** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Log\\_Grid} = \\left\\lfloor \\frac{\\text{Log}}{\\Delta \\text{Log}} \\right\\rfloor \\times \\Delta \\text{Log}\n",
    "$$\n",
    "\n",
    "## **4. Sorting by Time and Device**\n",
    "To track movement **chronologically** for each vehicle, we sort:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "## **5. Identifying Stationary Periods**\n",
    "For each vehicle, we determine if it remained in the same grid cell over consecutive timestamps:\n",
    "\n",
    "$$\n",
    "\\text{Same\\_Grid}_i =\n",
    "\\begin{cases} \n",
    "1, & (\\text{Lat\\_Grid}_i = \\text{Lat\\_Grid}_{i-1}) \\land (\\text{Log\\_Grid}_i = \\text{Log\\_Grid}_{i-1}) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\text{Same\\_Grid}_i = 1 $ means no movement occurred.\n",
    "- $ \\text{Same\\_Grid}_i = 0 $ means movement occurred.\n",
    "\n",
    "## **6. Computing Time Spent in a Grid Cell**\n",
    "The time difference between consecutive records within the same grid is:\n",
    "\n",
    "$$\n",
    "\\Delta t_i = T_i - T_{i-1}\n",
    "$$\n",
    "\n",
    "The **total duration** a vehicle spends within a specific grid cell before moving is:\n",
    "\n",
    "$$\n",
    "\\text{Cumulative\\_Time}_{i} = \\sum_{k=1}^{i} \\Delta t_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The summation continues **until movement occurs**.\n",
    "\n",
    "## **7. Assigning a Group ID to Each Stationary Period**\n",
    "A **unique group identifier** is assigned to each stationary period using a cumulative sum:\n",
    "\n",
    "$$\n",
    "\\text{Group}_i =\n",
    "\\sum_{j=1}^{i} (1 - \\text{Same\\_Grid}_j)\n",
    "$$\n",
    "\n",
    "Each transition into a **new grid cell** increments the group ID.\n",
    "\n",
    "## **8. Removing Prolonged Stationary Vehicles**\n",
    "Vehicles remaining in the **same grid for over 3 hours** (10,800 seconds) are excluded:\n",
    "\n",
    "$$\n",
    "\\text{Remove } i \\text{ if } \\text{Cumulative\\_Time}_i \\geq 10,800 \\text{ sec}\n",
    "$$\n",
    "\n",
    "## **9. Cleanup**\n",
    "All intermediate columns used for calculations are dropped to optimize storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# Ensure 'Lat' and 'Log' are numeric\n",
    "df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "df['Log'] = pd.to_numeric(df['Log'], errors='coerce')\n",
    "\n",
    "# Spatial Resolution: 120m grid\n",
    "grid=120\n",
    "lat_resolution = grid / 111320  \n",
    "df['Lat_Grid'] = (df['Lat'] // lat_resolution) * lat_resolution\n",
    "\n",
    "# Longitude resolution depends on latitude\n",
    "df['Lon_Resolution'] = grid / (111320 * np.cos(np.radians(df['Lat'])))\n",
    "df['Log_Grid'] = (df['Log'] // df['Lon_Resolution']) * df['Lon_Resolution']\n",
    "\n",
    "# Drop auxiliary column\n",
    "df = df.drop(columns=['Lon_Resolution'])\n",
    "\n",
    "# Step 1: Sort by deviceID and Timestamp\n",
    "df = df.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 2: Detect continuous stationary periods\n",
    "df['Prev_Lat_Grid'] = df.groupby('deviceID')['Lat_Grid'].shift(1)\n",
    "df['Prev_Log_Grid'] = df.groupby('deviceID')['Log_Grid'].shift(1)\n",
    "df['Prev_Timestamp'] = df.groupby('deviceID')['Timestamp'].shift(1)\n",
    "\n",
    "# Step 3: Identify whether the taxi has stayed in the same grid\n",
    "df['Same_Grid'] = (df['Lat_Grid'] == df['Prev_Lat_Grid']) & (df['Log_Grid'] == df['Prev_Log_Grid'])\n",
    "\n",
    "# Step 4: Compute time spent in the grid continuously\n",
    "df['Time_Diff'] = (df['Timestamp'] - df['Prev_Timestamp']).dt.total_seconds()\n",
    "\n",
    "# Step 5: Assign a group ID that resets when the taxi leaves a grid\n",
    "df['Group'] = (~df['Same_Grid']).cumsum()\n",
    "\n",
    "# Step 6: Compute total time spent in each visit to the grid\n",
    "df['Cumulative_Time'] = df.groupby(['deviceID', 'Lat_Grid', 'Log_Grid', 'Group'])['Time_Diff'].cumsum()\n",
    "\n",
    "# Step 7: Remove vehicles that stayed continuously in the same grid for more than 3hours (10800 sec)\n",
    "df = df[~(df['Cumulative_Time'] >= 10800)]\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['Prev_Lat_Grid', 'Prev_Log_Grid', 'Prev_Timestamp', 'Same_Grid', 'Time_Diff', 'Group', 'Cumulative_Time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Aggregation\n",
    "\n",
    "This script generates a **120m x 120m geospatial grid** and counts the number of data points within each grid cell:\n",
    "\n",
    "1. **Dynamic Boundary Definition**: \n",
    "   - Extracts min/max latitude and longitude from the dataset.\n",
    "2. **Grid Construction**:\n",
    "   - Defines **120m resolution** for latitude and dynamically calculates longitude resolution.\n",
    "   - Iterates over the spatial extent to generate **polygonal grid cells**.\n",
    "3. **GeoDataFrame Creation**:\n",
    "   - Converts the grid into a `GeoDataFrame` (`grid_gdf`).\n",
    "   - Converts data points into a `GeoDataFrame` (`df_gdf`).\n",
    "4. **Spatial Aggregation**:\n",
    "   - Checks which points fall within each grid cell.\n",
    "   - Increments the count of data points within corresponding grid polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically define the bounds from the DataFrame\n",
    "min_lat = df['Lat'].min()\n",
    "max_lat = df['Lat'].max()\n",
    "min_lon = df['Log'].min()\n",
    "max_lon = df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Approximate latitude resolution\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid of polygons\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the points in df\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Increment the count for each grid square containing points\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Visualization of Measurement Density\n",
    "\n",
    "Visualize the spatial distribution of measurements using a **120m x 120m grid heatmap**:\n",
    "\n",
    "1. **Fractional Power Scaling**:\n",
    "   - Applies **gamma correction (0.3)** to the count data for better visibility (can be modified for viz purposes).\n",
    "   - Ensures variations in density are more distinguishable.\n",
    "\n",
    "2. **Grid Heatmap Plotting**:\n",
    "   - Colors the grid cells based on scaled count values using the **jet colormap**.\n",
    "   - Adjusts transparency (`alpha=0.4`) for better layering.\n",
    "\n",
    "3. **Basemap Integration**:\n",
    "   - Overlays the heatmap on an **OpenStreetMap** basemap for context.\n",
    "\n",
    "4. **Plot Aesthetics**:\n",
    "   - Titles the plot **\"Heatmap of Measurements Across Stockholm (120x120m Grid)\"**.\n",
    "   - Removes axis labels for cleaner visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Fractional Power Scaling to 'Count'\n",
    "gamma = 0.3  # Adjust between 0.2 - 0.5 for better visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "\n",
    "# Plot the grid using the scaled count with a jet colormap\n",
    "grid_gdf.plot(\n",
    "    column='Scaled_Count',\n",
    "    cmap='jet',\n",
    "    legend=True,\n",
    "    alpha=0.4,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Add a basemap\n",
    "ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Adjust plot aesthetics\n",
    "ax.set_title('Heatmap of Measurements Across Stockholm (120x120m Grid)', fontsize=12)\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Frequency Analysis in High-Density Grid Cells\n",
    "\n",
    "This script analyzes the **sampling frequency** in the **top 10 most frequently sensed grid cells**:\n",
    "\n",
    "1. **Identify High-Activity Areas**:\n",
    "   - Selects the **top 10 grid cells** with the highest measurement counts.\n",
    "\n",
    "2. **Filter Relevant Data**:\n",
    "   - Extracts sensor readings from these **top 10 grid cells**.\n",
    "\n",
    "3. **Compute Time Differences**:\n",
    "   - Calculates the time intervals between consecutive measurements per `deviceID` within each grid cell.\n",
    "\n",
    "4. **Estimate Sampling Frequency**:\n",
    "   - Computes the **mean sampling interval** per grid cell.\n",
    "   - Converts it to **sampling frequency (Hz)** as **1 / mean time difference**.\n",
    "\n",
    "5. **Extract Top Sampling Frequencies**:\n",
    "   - Selects the **10 highest sampling frequencies** for analysis.\n",
    "   - Converts results into a structured DataFrame for better readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 10 most frequently sensed grid cells\n",
    "top_10_cells = grid_gdf.nlargest(10, 'Count')\n",
    "\n",
    "# Filter the data for points within these top 10 grid cells\n",
    "df_top_cells = df_gdf[df_gdf.geometry.apply(lambda point: any(top_10_cells.contains(point)))].copy()  # Make a copy\n",
    "\n",
    "# Compute the time differences for each device in each grid cell\n",
    "df_top_cells['Time_Diff'] = df_top_cells.groupby(['Lat_Grid', 'Log_Grid', 'deviceID'])['Timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "# Calculate the mean sampling frequency (1 / mean time difference) per grid cell\n",
    "sampling_frequency = df_top_cells.groupby(['Lat_Grid', 'Log_Grid'])['Time_Diff'].mean().dropna().apply(lambda x: 1 / x)\n",
    "top_10_frequencies = sampling_frequency.nlargest(10)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "top_10_frequencies_df = top_10_frequencies.reset_index()\n",
    "top_10_frequencies_df.columns = ['Lat_Grid', 'Log_Grid', 'Sampling_Frequency (Hz)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Battery Depletion\n",
    "\n",
    "Identify the **number of unique days** where at least one deviceâ€™s battery **(SOC_batt)** dropped below **50%**:\n",
    "\n",
    "1. **Extract Date Information**:\n",
    "   - Converts the timestamp to **date-only format**.\n",
    "\n",
    "2. **Filter for Battery Depletion Events**:\n",
    "   - Selects records where `SOC_batt` is below **50%** (can be changed).\n",
    "\n",
    "3. **Count Unique Affected Days**:\n",
    "   - Computes the number of distinct days where a depletion event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded with the necessary data\n",
    "# Identify unique days where at least one device's SOC_batt dropped below 50%\n",
    "df['Date'] = df['Timestamp'].dt.date  # Extract the date\n",
    "days_with_depletion = df[df['SOC_batt'] < 50]['Date'].nunique()\n",
    "\n",
    "# Display the number of days with a battery drop below 50%\n",
    "days_with_depletion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov-Based Spatial Transition Modeling for Vehicle Movement and Sensor Depletion Analysis**\n",
    "\n",
    "This methodology enables **data-driven mobility prediction** and **battery depletion analysis** using **Markovian dynamics**.\n",
    "\n",
    "## **1. Assigning Vehicles to Grid Cells Using Polygon Containment**\n",
    "Given a dataset of GPS points (`Lat`, `Log`), we spatially bin the data into a **120m x 120m** grid using polygon containment:\n",
    "\n",
    "- Convert each GPS point into a **GeoDataFrame** (`df_gdf`).\n",
    "- Each point is assigned to its corresponding grid cell using the spatial containment function:\n",
    "\n",
    "  $$\n",
    "  G(i) = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ p_i $ is the point geometry of record $ i $,\n",
    "  - $ P_j $ is the polygon geometry of grid cell $ j $,\n",
    "  - $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $, else **0**.\n",
    "\n",
    "- Any points that **do not match** a grid cell are treated as **outliers** and removed.\n",
    "\n",
    "## **2. Temporal Sorting for Transition Analysis**\n",
    "To analyze transitions between grid cells, the dataset is **sorted** by:\n",
    "\n",
    "$$\n",
    "\\text{Sort}(df, \\text{by}=[\\text{deviceID}, \\text{Timestamp}])\n",
    "$$\n",
    "\n",
    "where:\n",
    "- `deviceID` ensures sorting is done **per vehicle**,\n",
    "- `Timestamp` orders data points **chronologically**.\n",
    "\n",
    "## **3. Identifying Sensor Depletion Events**\n",
    "A **battery depletion threshold** is defined as:\n",
    "\n",
    "$$\n",
    "SOC_{\\text{batt}} < 50\\%\n",
    "$$\n",
    "\n",
    "where **State of Charge (SOC)** is the batteryâ€™s remaining capacity. We create a binary indicator:\n",
    "\n",
    "$$\n",
    "D_i = \n",
    "\\begin{cases} \n",
    "1, & SOC_{\\text{batt}, i} < 50\\% \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Pre-Depletion Data**: $ D_i = 0 $ (battery level above threshold).\n",
    "- **Post-Depletion Data**: $ D_i = 1 $ (battery level below threshold).\n",
    "\n",
    "## **4. Constructing Grid Cell Transitions**\n",
    "To model **spatial movement**, we define a transition as:\n",
    "\n",
    "$$\n",
    "T_i = (G_i, G_{i+1})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the grid cell at time $ t_i $,\n",
    "- $ G_{i+1} $ is the **next** grid cell at $ t_{i+1} $.\n",
    "\n",
    "We compute the **state transitions** for each vehicle:\n",
    "\n",
    "$$\n",
    "\\text{Next\\_Grid\\_Cell} = G_{i+1} = \\text{shift}(G_i, -1)\n",
    "$$\n",
    "\n",
    "Dropping the last row for each vehicle ensures only **valid transitions** are included.\n",
    "\n",
    "## **5. Constructing the Markov Transition Matrix**\n",
    "A **first-order Markov model** is constructed, where each transition probability is estimated as:\n",
    "\n",
    "$$\n",
    "P(G_{i+1} | G_i) = \\frac{N(G_i \\to G_{i+1})}{\\sum_{G_j} N(G_i \\to G_j)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N(G_i \\to G_{i+1}) $ is the **count** of observed transitions from $ G_i $ to $ G_{i+1} $,\n",
    "- The denominator sums over **all possible** next states.\n",
    "\n",
    "The **transition matrix** $ P $ is structured as:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "P(G_1 | G_1) & P(G_2 | G_1) & \\dots & P(G_n | G_1) \\\\\n",
    "P(G_1 | G_2) & P(G_2 | G_2) & \\dots & P(G_n | G_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P(G_1 | G_n) & P(G_2 | G_n) & \\dots & P(G_n | G_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a **stochastic matrix**, where each row sums to **1**:\n",
    "\n",
    "$$\n",
    "\\sum_{G_{i+1}} P(G_{i+1} | G_i) = 1, \\quad \\forall G_i\n",
    "$$\n",
    "\n",
    "## **6. Predicting the Most Likely Next Grid Cell**\n",
    "For a given grid cell $ G_i $, the predicted **next location** is:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}} = \\arg\\max_{G_{i+1}} P(G_{i+1} | G_i)\n",
    "$$\n",
    "\n",
    "This follows a **greedy decision rule**, selecting the most probable transition based on historical data.\n",
    "\n",
    "## **7. Validation Against Post-Depletion Data**\n",
    "For vehicles that **experienced battery depletion**:\n",
    "\n",
    "- The predicted transition is compared to the **actual** next grid cell.\n",
    "- A **correct prediction** is counted if:\n",
    "\n",
    "  $$\n",
    "  G_{\\text{predicted}} = G_{\\text{actual}}\n",
    "  $$\n",
    "\n",
    "- The **prediction accuracy** is computed as:\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\sum \\mathbb{1}(G_{\\text{predicted}} = G_{\\text{actual}})}{N_{\\text{post-depletion}}}\n",
    "  $$\n",
    "\n",
    "where $ N_{\\text{post-depletion}} $ is the total number of post-depletion data points.\n",
    "\n",
    "## **8. Output and Insights**\n",
    "- The **transition matrix** is saved for further analysis.\n",
    "- The **post-depletion validation results** are stored.\n",
    "- The **top 10 most likely transitions** are displayed, providing insight into dominant movement patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1ST ORDER MC\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "depletion_threshold = 50\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create Transitions (from one grid cell to the next)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop last row per vehicle (no next transition available)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build the Markov Transition Matrix\n",
    "transition_counts = (\n",
    "    df_transitions.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities = transition_counts.div(transition_counts.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# Step 7: Define a Function to Predict the Next Grid Cell\n",
    "def predict_next_grid(current_grid, transition_matrix):\n",
    "    if current_grid in transition_matrix.index:\n",
    "        return transition_matrix.loc[current_grid].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 8: Validate Predictions Using Post-Depletion Data\n",
    "df_post_depletion['Predicted_Grid_Cell'] = df_post_depletion['Grid_Cell'].apply(\n",
    "    lambda x: predict_next_grid(x, transition_probabilities)\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy\n",
    "df_post_depletion['Correct_Prediction'] = df_post_depletion['Predicted_Grid_Cell'] == df_post_depletion['Grid_Cell']\n",
    "accuracy = df_post_depletion['Correct_Prediction'].mean()\n",
    "\n",
    "print(f\"Prediction Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# # Step 10: Save the Transition Matrix and Post-Depletion Validation\n",
    "# output_path_transition_matrix = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix.xlsx\"\n",
    "# transition_probabilities.to_excel(output_path_transition_matrix)\n",
    "\n",
    "# output_path_post_depletion = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion, index=False)\n",
    "\n",
    "# # Step 11: Analyze and Print the Top 10 Most Likely Transitions\n",
    "# most_likely_transitions = (\n",
    "#     transition_probabilities.stack()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={0: 'Probability', 'level_0': 'From_Grid', 'level_1': 'To_Grid'})\n",
    "#     .sort_values(by='Probability', ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"Top 10 Most Likely Transitions:\")\n",
    "# print(most_likely_transitions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ND ORDER MC\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "depletion_threshold = 50\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create Second-Order Transitions (considering two previous grid cells)\n",
    "df_pre_depletion['Prev_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop rows where not enough history is available\n",
    "df_transitions_2nd = df_pre_depletion.dropna(subset=['Prev_Grid_Cell', 'Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build the Second-Order Transition Matrix\n",
    "transition_counts_2nd = (\n",
    "    df_transitions_2nd.groupby(['Prev_Grid_Cell', 'Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities_2nd = transition_counts_2nd.div(transition_counts_2nd.sum(axis=1), axis=0)  # Normalize to probabilities\n",
    "\n",
    "# Step 7: Define Prediction Function for Second-Order Markov Chain\n",
    "def predict_next_grid_2nd(prev_grid, current_grid, transition_matrix):\n",
    "    key = (prev_grid, current_grid)\n",
    "    if key in transition_matrix.index:\n",
    "        return transition_matrix.loc[key].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 8: Apply Second-Order Prediction to Post-Depletion Data\n",
    "df_post_depletion['Prev_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "df_post_depletion['Predicted_Grid_Cell_2nd'] = df_post_depletion.apply(\n",
    "    lambda row: predict_next_grid_2nd(row['Prev_Grid_Cell'], row['Grid_Cell'], transition_probabilities_2nd), axis=1\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy for Second-Order Markov Chain\n",
    "df_post_depletion['Correct_Prediction_2nd'] = df_post_depletion['Predicted_Grid_Cell_2nd'] == df_post_depletion['Grid_Cell']\n",
    "accuracy_2nd_order = df_post_depletion['Correct_Prediction_2nd'].mean()\n",
    "\n",
    "print(f\"Second-Order Markov Chain Prediction Accuracy: {accuracy_2nd_order:.2%}\")\n",
    "\n",
    "# # Step 10: Save the Second-Order Transition Matrix and Post-Depletion Validation\n",
    "# output_path_transition_matrix_2nd = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix_2nd_Order.xlsx\"\n",
    "# transition_probabilities_2nd.to_excel(output_path_transition_matrix_2nd)\n",
    "\n",
    "# output_path_post_depletion_2nd = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation_2nd.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion_2nd, index=False)\n",
    "\n",
    "# # Step 11: Analyze and Print the Top 10 Most Likely Transitions for the Second-Order Markov Chain\n",
    "# most_likely_transitions_2nd = (\n",
    "#     transition_probabilities_2nd.stack()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={0: 'Probability', 'level_0': 'Prev_Grid', 'level_1': 'Current_Grid', 'level_2': 'To_Grid'})\n",
    "#     .sort_values(by='Probability', ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"Top 10 Most Likely Transitions (Second-Order Markov Chain):\")\n",
    "# print(most_likely_transitions_2nd.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYBRID 1ST & 2ND ORDER MC\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "depletion_threshold = 50\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Create First-Order Transitions\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Create Second-Order Transitions (considering two previous grid cells)\n",
    "df_pre_depletion['Prev_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "\n",
    "# Drop rows where not enough history is available\n",
    "df_transitions_1st = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "df_transitions_2nd = df_pre_depletion.dropna(subset=['Prev_Grid_Cell', 'Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Build Transition Matrices\n",
    "## First-Order Markov Chain\n",
    "transition_counts_1st = (\n",
    "    df_transitions_1st.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities_1st = transition_counts_1st.div(transition_counts_1st.sum(axis=1), axis=0)\n",
    "\n",
    "## Second-Order Markov Chain\n",
    "transition_counts_2nd = (\n",
    "    df_transitions_2nd.groupby(['Prev_Grid_Cell', 'Grid_Cell', 'Next_Grid_Cell'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "transition_probabilities_2nd = transition_counts_2nd.div(transition_counts_2nd.sum(axis=1), axis=0)\n",
    "\n",
    "# Step 7: Define Hybrid Prediction Function\n",
    "def predict_next_grid_hybrid(prev_grid, current_grid, transition_matrix_1st, transition_matrix_2nd):\n",
    "    key = (prev_grid, current_grid)\n",
    "    \n",
    "    # Use Second-Order if available\n",
    "    if key in transition_matrix_2nd.index:\n",
    "        return transition_matrix_2nd.loc[key].idxmax()  # Most likely transition\n",
    "    \n",
    "    # If Second-Order not available, use First-Order\n",
    "    elif current_grid in transition_matrix_1st.index:\n",
    "        return transition_matrix_1st.loc[current_grid].idxmax()\n",
    "    \n",
    "    # If neither available, return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Step 8: Apply Hybrid Prediction to Post-Depletion Data\n",
    "df_post_depletion['Prev_Grid_Cell'] = df_post_depletion.groupby('deviceID')['Grid_Cell'].shift(1)\n",
    "df_post_depletion['Predicted_Grid_Cell_Hybrid'] = df_post_depletion.apply(\n",
    "    lambda row: predict_next_grid_hybrid(row['Prev_Grid_Cell'], row['Grid_Cell'], transition_probabilities_1st, transition_probabilities_2nd), axis=1\n",
    ")\n",
    "\n",
    "# Step 9: Measure Prediction Accuracy for Hybrid Model\n",
    "df_post_depletion['Correct_Prediction_Hybrid'] = df_post_depletion['Predicted_Grid_Cell_Hybrid'] == df_post_depletion['Grid_Cell']\n",
    "accuracy_hybrid = df_post_depletion['Correct_Prediction_Hybrid'].mean()\n",
    "\n",
    "print(f\"Hybrid Markov Chain Prediction Accuracy: {accuracy_hybrid:.2%}\")\n",
    "\n",
    "# # Step 10: Save the Hybrid Transition Matrices and Post-Depletion Validation\n",
    "# output_path_transition_matrix_1st = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix_1st_Order.xlsx\"\n",
    "# transition_probabilities_1st.to_excel(output_path_transition_matrix_1st)\n",
    "\n",
    "# output_path_transition_matrix_2nd = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix_2nd_Order.xlsx\"\n",
    "# transition_probabilities_2nd.to_excel(output_path_transition_matrix_2nd)\n",
    "\n",
    "# output_path_post_depletion_hybrid = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation_Hybrid.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion_hybrid, index=False)\n",
    "\n",
    "# # Step 11: Analyze and Print the Top 10 Most Likely Transitions for Hybrid Markov Chain\n",
    "# most_likely_transitions_hybrid = (\n",
    "#     transition_probabilities_2nd.stack()\n",
    "#     .reset_index()\n",
    "#     .rename(columns={0: 'Probability', 'level_0': 'Prev_Grid', 'level_1': 'Current_Grid', 'level_2': 'To_Grid'})\n",
    "#     .sort_values(by='Probability', ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"Top 10 Most Likely Transitions (Hybrid Markov Chain):\")\n",
    "# print(most_likely_transitions_hybrid.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME-BASED MC\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()  # Return index of matching grid cell\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "depletion_threshold = 50\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Extract Time-of-Day Bins\n",
    "df_gdf['Hour'] = df_gdf['Timestamp'].dt.hour\n",
    "df_gdf['Time_Bin'] = pd.cut(\n",
    "    df_gdf['Hour'],\n",
    "    bins=[0, 6, 12, 18, 24],  # Defining the four time periods\n",
    "    labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
    "    right=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Step 5: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 6: Create First-Order Transitions Within Each Time Bin\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby(['deviceID', 'Time_Bin'])['Grid_Cell'].shift(-1)\n",
    "\n",
    "# Drop last row per vehicle per time bin (no next transition available)\n",
    "df_transitions_time = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 7: Build Time-Based Transition Matrices\n",
    "transition_matrices_time = {}\n",
    "\n",
    "for time_bin in df_transitions_time['Time_Bin'].unique():\n",
    "    df_time_bin = df_transitions_time[df_transitions_time['Time_Bin'] == time_bin]\n",
    "    \n",
    "    transition_counts_time = (\n",
    "        df_time_bin.groupby(['Grid_Cell', 'Next_Grid_Cell'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    \n",
    "    transition_matrices_time[time_bin] = transition_counts_time.div(transition_counts_time.sum(axis=1), axis=0)\n",
    "\n",
    "# Step 8: Define Time-Based Prediction Function\n",
    "def predict_next_grid_time_based(current_grid, time_bin, transition_matrices):\n",
    "    if time_bin in transition_matrices and current_grid in transition_matrices[time_bin].index:\n",
    "        return transition_matrices[time_bin].loc[current_grid].idxmax()  # Most likely transition\n",
    "    else:\n",
    "        return None  # No transition data available\n",
    "\n",
    "# Step 9: Apply Time-Based Prediction to Post-Depletion Data\n",
    "df_post_depletion['Predicted_Grid_Cell_Time_Based'] = df_post_depletion.apply(\n",
    "    lambda row: predict_next_grid_time_based(row['Grid_Cell'], row['Time_Bin'], transition_matrices_time), axis=1\n",
    ")\n",
    "\n",
    "# Step 10: Measure Prediction Accuracy for Time-Based Markov Chain\n",
    "df_post_depletion['Correct_Prediction_Time_Based'] = df_post_depletion['Predicted_Grid_Cell_Time_Based'] == df_post_depletion['Grid_Cell']\n",
    "accuracy_time_based = df_post_depletion['Correct_Prediction_Time_Based'].mean()\n",
    "\n",
    "print(f\"Time-Based Markov Chain Prediction Accuracy: {accuracy_time_based:.2%}\")\n",
    "\n",
    "# # Step 11: Save the Time-Based Transition Matrices and Post-Depletion Validation\n",
    "# for time_bin, matrix in transition_matrices_time.items():\n",
    "#     output_path = f\"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Transition_Matrix_{time_bin}.xlsx\"\n",
    "#     matrix.to_excel(output_path)\n",
    "\n",
    "# output_path_post_depletion_time = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/Post_Depletion_Validation_Time_Based.xlsx\"\n",
    "# df_post_depletion.to_excel(output_path_post_depletion_time, index=False)\n",
    "\n",
    "# # Step 12: Analyze and Print the Top 10 Most Likely Transitions for Time-Based Markov Chain\n",
    "# most_likely_transitions_time_based = {}\n",
    "\n",
    "# for time_bin, matrix in transition_matrices_time.items():\n",
    "#     most_likely_transitions_time_based[time_bin] = (\n",
    "#         matrix.stack()\n",
    "#         .reset_index()\n",
    "#         .rename(columns={0: 'Probability', 'level_0': 'Current_Grid', 'level_1': 'To_Grid'})\n",
    "#         .sort_values(by='Probability', ascending=False)\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Top 10 Most Likely Transitions ({time_bin}):\")\n",
    "#     print(most_likely_transitions_time_based[time_bin].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# Step 1: Assign Vehicles to Grid Cells Using Polygon Containment\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Function to find which grid cell a point belongs to\n",
    "def find_grid_cell(point, grid_gdf):\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        return match.idxmax()\n",
    "    else:\n",
    "        return None  # No match found\n",
    "\n",
    "# Apply function to assign each GPS point to a grid cell\n",
    "df_gdf['Grid_Cell'] = df_gdf.apply(lambda row: find_grid_cell(row, grid_gdf), axis=1)\n",
    "\n",
    "# Drop rows where no grid cell was matched (outliers)\n",
    "df_gdf = df_gdf.dropna(subset=['Grid_Cell'])\n",
    "\n",
    "# Step 2: Sort by deviceID and Timestamp for Transition Analysis\n",
    "df_gdf = df_gdf.sort_values(by=['deviceID', 'Timestamp'])\n",
    "\n",
    "# Step 3: Identify Sensor Depletion (SOC_batt < 50)\n",
    "depletion_threshold = 50\n",
    "df_gdf['Depleted'] = df_gdf['SOC_batt'] < depletion_threshold\n",
    "\n",
    "# Step 4: Separate Pre- and Post-Depletion Data\n",
    "df_pre_depletion = df_gdf[~df_gdf['Depleted']].copy()\n",
    "df_post_depletion = df_gdf[df_gdf['Depleted']].copy()\n",
    "\n",
    "# Step 5: Extract Transitions (State, Action, Next State)\n",
    "df_pre_depletion['Next_Grid_Cell'] = df_pre_depletion.groupby('deviceID')['Grid_Cell'].shift(-1)\n",
    "df_transitions = df_pre_depletion.dropna(subset=['Next_Grid_Cell'])\n",
    "\n",
    "# Step 6: Define RL Training Function for Hyperparameter Optimization\n",
    "def train_rl(params):\n",
    "    alpha = params['alpha']\n",
    "    gamma = params['gamma']\n",
    "    epsilon = params['epsilon']\n",
    "    epsilon_decay = params['epsilon_decay']\n",
    "    training_iterations = int(params['training_iterations'])\n",
    "\n",
    "    # Initialize Q-Table\n",
    "    q_table = {}\n",
    "\n",
    "    # RL Training with Dynamic Exploration and Replay\n",
    "    for _ in range(training_iterations):\n",
    "        for _, row in df_transitions.iterrows():\n",
    "            state = row['Grid_Cell']\n",
    "            action = row['Next_Grid_Cell']\n",
    "\n",
    "            if state not in q_table:\n",
    "                q_table[state] = {}\n",
    "\n",
    "            if action not in q_table[state]:\n",
    "                q_table[state][action] = 0\n",
    "\n",
    "            # Log-based reward normalization\n",
    "            transition_count = df_transitions[(df_transitions['Grid_Cell'] == state) & (df_transitions['Next_Grid_Cell'] == action)].shape[0]\n",
    "            reward = np.log(transition_count + 1)\n",
    "\n",
    "            # Q-learning update\n",
    "            max_future_q = max(q_table[state].values()) if q_table[state] else 0\n",
    "            q_table[state][action] = (1 - alpha) * q_table[state][action] + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "    # RL Prediction Function with Decaying Exploration\n",
    "    def predict_next_grid_rl(state, q_table, epsilon):\n",
    "        if state in q_table:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                return random.choice(list(q_table[state].keys()))  # Explore\n",
    "            else:\n",
    "                return max(q_table[state], key=q_table[state].get)  # Exploit\n",
    "        return None\n",
    "\n",
    "    # Apply RL-Based Prediction to Post-Depletion Data\n",
    "    df_post_depletion['Predicted_Grid_Cell_RL'] = df_post_depletion['Grid_Cell'].apply(lambda x: predict_next_grid_rl(x, q_table, epsilon))\n",
    "\n",
    "    # Reduce Epsilon Over Time\n",
    "    epsilon = max(0.05, epsilon * epsilon_decay)\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    df_post_depletion['Correct_Prediction_RL'] = df_post_depletion['Predicted_Grid_Cell_RL'] == df_post_depletion['Grid_Cell']\n",
    "    accuracy = df_post_depletion['Correct_Prediction_RL'].mean()\n",
    "\n",
    "    print(f\"Trial Accuracy: {accuracy:.2%} | Params: {params}\")\n",
    "\n",
    "    # Stop if accuracy reaches 85%\n",
    "    if accuracy >= 0.85:\n",
    "        print(\"Target accuracy reached! Stopping optimization.\")\n",
    "        return {'loss': -accuracy, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Step 7: Define Hyperparameter Search Space\n",
    "param_space = {\n",
    "    'alpha': hp.uniform('alpha', 0.1, 0.5),  # Learning rate (0.1 - 0.5)\n",
    "    'gamma': hp.uniform('gamma', 0.5, 0.9),  # Discount factor (0.5 - 0.9)\n",
    "    'epsilon': hp.uniform('epsilon', 0.1, 0.3),  # Exploration rate (10%-30%)\n",
    "    'epsilon_decay': hp.uniform('epsilon_decay', 0.98, 0.999),  # Slower decay (98%-99.9%)\n",
    "    'training_iterations': hp.quniform('training_iterations', 2, 10, 1)  # Replay transitions (2-10 times)\n",
    "}\n",
    "\n",
    "# Step 8: Run Hyperparameter Optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=train_rl, space=param_space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(f\"Best RL Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Post-Depletion Data to GeoDataFrame for Visualization\n",
    "# df_post_depletion = gpd.GeoDataFrame(df_post_depletion, \n",
    "#                                      geometry=gpd.points_from_xy(df_post_depletion['Log_Grid'], df_post_depletion['Lat_Grid']), \n",
    "#                                      crs=\"EPSG:4326\")\n",
    "\n",
    "# # Convert Predicted Grid Cells to Points\n",
    "# predicted_points = df_post_depletion[['Predicted_Grid_Cell']].dropna().copy()\n",
    "# predicted_points['Lat_Pred'] = predicted_points['Predicted_Grid_Cell'].apply(lambda x: grid_gdf.loc[x, 'geometry'].centroid.y if x in grid_gdf.index else None)\n",
    "# predicted_points['Lon_Pred'] = predicted_points['Predicted_Grid_Cell'].apply(lambda x: grid_gdf.loc[x, 'geometry'].centroid.x if x in grid_gdf.index else None)\n",
    "# predicted_points = predicted_points.dropna()\n",
    "\n",
    "# # Convert to GeoDataFrame\n",
    "# gdf_predicted = gpd.GeoDataFrame(predicted_points, \n",
    "#                                  geometry=gpd.points_from_xy(predicted_points['Lon_Pred'], predicted_points['Lat_Pred']), \n",
    "#                                  crs=\"EPSG:4326\")\n",
    "\n",
    "# # Plot the actual and predicted trajectories\n",
    "# fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# # Plot Grid\n",
    "# grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.4)\n",
    "\n",
    "# # Plot Actual Trajectory (Post-Depletion)\n",
    "# df_post_depletion.plot(ax=ax, color='red', markersize=20, label=\"Actual Trajectory\", alpha=0.7)\n",
    "\n",
    "# # Plot Predicted Trajectory\n",
    "# gdf_predicted.plot(ax=ax, color='blue', markersize=20, label=\"Predicted Trajectory\", alpha=0.7)\n",
    "\n",
    "# # Add Basemap\n",
    "# ctx.add_basemap(ax, crs=df_post_depletion.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# # Formatting\n",
    "# ax.set_title(\"Actual vs Predicted Trajectory (Post-Depletion)\", fontsize=14)\n",
    "# ax.legend()\n",
    "# ax.set_axis_off()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mathematical Formulation of Post-Depletion Trajectory Analysis**\n",
    "\n",
    "This analysis enables a detailed comparison of **actual vs predicted movement** following battery depletion, revealing:\n",
    "- How vehicles move after depletion.\n",
    "- The effectiveness of the **Markov-based prediction model**.\n",
    "- Patterns in vehicle trajectory shifts post-depletion.\n",
    "\n",
    "## **1. Extracting Unique Days of Battery Depletion**\n",
    "We define a unique day $ d $ as a calendar date where at least one vehicle experienced battery depletion:\n",
    "\n",
    "$$\n",
    "D = \\{ d \\mid \\exists i, SOC_{\\text{batt}, i} < 50\\%, \\text{on day } d \\}\n",
    "$$\n",
    "\n",
    "where $ D $ is the set of all days in which a depletion event occurred.\n",
    "\n",
    "## **2. Filtering Data for Each Depletion Day**\n",
    "For each $ d \\in D $, we extract:\n",
    "\n",
    "- **Post-depletion records**: \n",
    "  $$\n",
    "  X_d = \\{ x_i \\mid x_i \\in X, \\text{Timestamp}(x_i) = d, SOC_{\\text{batt}, i} < 50\\% \\}\n",
    "  $$\n",
    "\n",
    "- **Pre-depletion records**:\n",
    "  $$\n",
    "  Y_d = \\{ y_i \\mid y_i \\in Y, \\text{Timestamp}(y_i) = d, SOC_{\\text{batt}, i} \\geq 50\\% \\}\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $ X_d $ represents all vehicle records **after depletion**.\n",
    "- $ Y_d $ represents all vehicle records **before depletion**.\n",
    "\n",
    "## **3. Mapping Vehicles to Grid Cells**\n",
    "For each vehicle's recorded GPS point $ p_i $ on day $ d $:\n",
    "\n",
    "$$\n",
    "G_i = \\arg\\max_j \\mathbb{1}(p_i \\in P_j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G_i $ is the assigned grid cell.\n",
    "- $ P_j $ represents the grid cells.\n",
    "- $ \\mathbb{1}(p_i \\in P_j) $ is an indicator function that is **1** if $ p_i $ is inside $ P_j $.\n",
    "\n",
    "Thus, we define:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{actual}} = \\{ G_i \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_d^{\\text{pre}} = \\{ G_i \\mid y_i \\in Y_d \\}\n",
    "$$\n",
    "\n",
    "## **4. Extracting Predicted Grid Cells**\n",
    "The predicted post-depletion grid cells for each vehicle are computed from the **Markov Transition Model**:\n",
    "\n",
    "$$\n",
    "G_{\\text{predicted}, i} = \\arg\\max_{G_j} P(G_j | G_i)\n",
    "$$\n",
    "\n",
    "for each vehicle location $ G_i $ before depletion.\n",
    "\n",
    "The predicted set is:\n",
    "\n",
    "$$\n",
    "G_d^{\\text{predicted}} = \\{ G_{\\text{predicted}, i} \\mid x_i \\in X_d \\}\n",
    "$$\n",
    "\n",
    "## **5. Visualizing the Spatial Trajectories**\n",
    "We generate a geospatial plot for each day $ d $:\n",
    "\n",
    "- **Pre-Depletion Trajectory** $ G_d^{\\text{pre}} $ (Black)\n",
    "- **Actual Post-Depletion Trajectory** $ G_d^{\\text{actual}} $ (Red)\n",
    "- **Predicted Trajectory** $ G_d^{\\text{predicted}} $ (Blue)\n",
    "\n",
    "Each grid cell is represented as a polygon, where:\n",
    "\n",
    "$$\n",
    "P_j = \\{ (x_k, y_k) \\mid k = 1,2,3,4 \\}\n",
    "$$\n",
    "\n",
    "and plotted based on its category:\n",
    "\n",
    "$$\n",
    "\\text{Color}(P_j) =\n",
    "\\begin{cases} \n",
    "\\text{black}, & P_j \\in G_d^{\\text{pre}} \\\\\n",
    "\\text{red}, & P_j \\in G_d^{\\text{actual}} \\\\\n",
    "\\text{blue}, & P_j \\in G_d^{\\text{predicted}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## **6. Overlaying the OpenStreetMap Basemap**\n",
    "The plotted grid cells are projected onto a real-world **OpenStreetMap** (OSM) basemap with coordinate reference system:\n",
    "\n",
    "$$\n",
    "\\text{CRS} = \\text{EPSG:4326}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day and generate a plot\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current day\n",
    "    df_day = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_pre_depletion_day = df_pre_depletion[df_pre_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert actual, predicted, and pre-depletion data into GeoDataFrames\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'\n",
    "\n",
    "    predicted_grid_cells = df_day['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'\n",
    "\n",
    "    pre_depletion_grid_cells = df_pre_depletion_day['Grid_Cell'].unique()\n",
    "    gdf_pre_depletion = grid_gdf.loc[grid_gdf.index.isin(pre_depletion_grid_cells)].copy()\n",
    "    gdf_pre_depletion['Color'] = 'black'\n",
    "\n",
    "    # Skip plotting if all GeoDataFrames are empty for the day\n",
    "    if gdf_actual.empty and gdf_predicted.empty and gdf_pre_depletion.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot for the current day\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black) if not empty\n",
    "    if not gdf_pre_depletion.empty:\n",
    "        gdf_pre_depletion.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Trajectory (Red) if not empty\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory\")\n",
    "\n",
    "    # Plot Predicted Trajectory (Blue) if not empty\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Actual vs Predicted Trajectory (Post-Depletion) - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy().reset_index(drop=True)  # Ensure indices are sequential\n",
    "df_temp=df_temp.sort_values(by=['Timestamp']).reset_index(drop=True) \n",
    "\n",
    "# Define different time thresholds to compare\n",
    "time_thresholds = {\n",
    "    \"3 sec\": 3,\n",
    "    \"12 sec\": 12\n",
    "}\n",
    "\n",
    "# Create a dictionary to store SOC and sensor states for each threshold\n",
    "soc_depletion_results = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "\n",
    "    # Track last sensed timestamp, and stored energy during OFF periods\n",
    "    last_sensed_time = {}\n",
    "    stored_energy = {}\n",
    "\n",
    "    # Previous date\n",
    "    prev_date=None\n",
    "    \n",
    "    for i in range(len(df_temp)):\n",
    "        row = df_temp.iloc[i]\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "        current_date = row['Date']\n",
    "        device= row['deviceID']\n",
    "\n",
    "        # Initialise inter-row differences when OFF\n",
    "        d_diff_prev=0 \n",
    "        \n",
    "        # Reset stored energy at the start of a new day\n",
    "        if prev_date is not None and current_date != prev_date:\n",
    "            stored_energy={}  # Reset stored energy for all grid cells\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}'] = 0  # Reset energy savings for the new day\n",
    "            print(f\"[RESET] Reset stored energy for new day: {current_date}\")\n",
    "\n",
    "        prev_date = current_date  # Update previous date tracker\n",
    "\n",
    "        if df_temp.loc[i, 'SOC_batt']>99:\n",
    "            stored_energy[grid_key]=0\n",
    "            df_temp.loc[i:, f'Energy_Saved_{label}']=0\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = False   \n",
    "\n",
    "            # Accumulate stored energy\n",
    "            if i > 0 and pd.notna(df_temp.iloc[i - 1]['SOC_batt']) and pd.notna(row['SOC_batt']):\n",
    "            \n",
    "                # Find the last preceding row for this device\n",
    "                if device == df_temp.iloc[i-1]['deviceID']:\n",
    "                    d_diff = max(0, df_temp.iloc[i - 1]['SOC_batt'] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"[OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "                else:\n",
    "                    d_diff = max(0, df_temp.loc[df_temp.deviceID == device, :]['SOC_batt'].iloc[-1] - row['SOC_batt']) #current inter-row difference\n",
    "                    diff = max(d_diff_prev, d_diff)\n",
    "                    stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                    stored_energy[grid_key] += diff\n",
    "                    \n",
    "                    if diff != 0: \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    else:\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "\n",
    "                    d_diff_prev=diff\n",
    "                    print(f\"CHANGE [OFF]: Accumulated {diff:.2f}% for device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "            df_temp.at[i, f'Sensor_ON_{label}'] = True\n",
    "            d_diff_prev=0\n",
    "\n",
    "            if device == df_temp.iloc[i-1]['deviceID']:\n",
    "\n",
    "                # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[i-1, f'Energy_Saved_{label}']\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]  \n",
    "                print(f\"[ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                 # Ensure stored_energy is initialized per grid cell without overwriting previous values\n",
    "                if grid_key not in stored_energy:\n",
    "                    if i > 0:\n",
    "                        #Carry forward the stored energy from the last known row\n",
    "                        stored_energy[grid_key] = df_temp.loc[df_temp.deviceID == device, :][f'Energy_Saved_{label}'].iloc[-1]\n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]\n",
    "                    elif i == 0:\n",
    "                        stored_energy[grid_key] = 0  # First iteration, no prior energy \n",
    "                        df_temp.loc[i:, f'Energy_Saved_{label}'] = stored_energy[grid_key]                 \n",
    "                \n",
    "                print(f\"CHANGE [ON]: device {device}. Total stored: {stored_energy[grid_key]:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Compute new SOC_batt with savings\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp['SOC_batt'] + df_temp[f'Energy_Saved_{label}']\n",
    "    df_temp[f'SOC_batt_{label}'] = df_temp[f'SOC_batt_{label}'].clip(upper=100)\n",
    "\n",
    "    # Compute SOC depletion for this threshold\n",
    "    daily_soc = df_temp.groupby(['Date', 'deviceID'])[f'SOC_batt_{label}'].mean()\n",
    "    soc_depletion_results[label] = daily_soc\n",
    "\n",
    "\n",
    "# Baseline: Compute SOC depletion without constraints\n",
    "soc_depletion_results[\"Baseline\"] = df_temp.groupby(['Date', 'deviceID'])['SOC_batt'].mean()\n",
    "\n",
    "# Convert results to a DataFrame for plotting\n",
    "soc_depletion_df = pd.DataFrame(soc_depletion_results)\n",
    "\n",
    "# Save the updated dataset with sensor states and energy savings for each threshold\n",
    "output_path = \"/Users/mayar/Desktop/MIT/Research_Fellow/ENERGY_SENSING/DATA/updated_SOC_batt_with_energy_savings.xlsx\"\n",
    "df_temp.to_excel(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line styles for each threshold\n",
    "line_styles = {\n",
    "    \"Baseline\": \"-\",\n",
    "    \"3 sec\": \"--\",\n",
    "    \"12 sec\": \":\"\n",
    "}\n",
    "\n",
    "# Predefined colors for devices\n",
    "predefined_colors = ['red', 'blue', 'green', 'black', 'purple']\n",
    "device_ids = set()\n",
    "for soc_series in soc_depletion_results.values():\n",
    "    device_ids.update(soc_series.index.get_level_values('deviceID').unique())\n",
    "\n",
    "# Create a color map using predefined colors\n",
    "color_map = {device_id: predefined_colors[i % len(predefined_colors)] for i, device_id in enumerate(sorted(device_ids))}\n",
    "\n",
    "# Plot SOC depletion for different devices and thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over thresholds and plot per device\n",
    "for label, soc_series in soc_depletion_results.items():  # soc_series is a MultiIndexed Series\n",
    "    for device_id in soc_series.index.get_level_values('deviceID').unique():  # Get unique devices\n",
    "        device_data = soc_series[soc_series.index.get_level_values('deviceID') == device_id]\n",
    "        plt.plot(\n",
    "            device_data.index.get_level_values('Date'),  # X-axis: Dates\n",
    "            device_data.values,  # Y-axis: SOC values\n",
    "            linestyle=line_styles[label],\n",
    "            color=color_map[device_id],  # Use predefined color for the device\n",
    "            marker='o',\n",
    "            markersize=3,\n",
    "            label=f\"Device {device_id} - {label}\"\n",
    "        )\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean SOC (%)')\n",
    "plt.title('SOC Depletion Comparison Across Devices and Time Constraints')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(1.05, 1),  # Place legend to the right of the plot\n",
    "    loc='upper left',          # Align legend to the top-left of the bounding box\n",
    "    borderaxespad=0.           # Reduce spacing between the legend and the plot\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the count of times the sensor was turned OFF for each constraint scenario\n",
    "off_counts = {}\n",
    "\n",
    "# Iterate over different time thresholds\n",
    "for label, TIME_THRESHOLD in time_thresholds.items():\n",
    "    df_copy = df.copy()  # Work on a copy of the dataset\n",
    "    df_copy['Sensor_ON'] = True  # Default: Sensor is ON\n",
    "\n",
    "    # Track last sensed timestamp per grid cell\n",
    "    last_sensed_time = {}\n",
    "    off_count = 0\n",
    "\n",
    "    for i, row in df_copy.iterrows():\n",
    "        grid_key = (row['Lat_Grid'], row['Log_Grid'])  # Unique grid identifier\n",
    "        current_time = row['Timestamp']\n",
    "\n",
    "        # If the grid was sensed recently (within the threshold), turn OFF the sensor\n",
    "        if grid_key in last_sensed_time and (current_time - last_sensed_time[grid_key]).total_seconds() < TIME_THRESHOLD:\n",
    "            df_copy.at[i, 'Sensor_ON'] = False\n",
    "            off_count += 1\n",
    "        else:\n",
    "            # Update last sensed time when the sensor turns ON\n",
    "            last_sensed_time[grid_key] = current_time\n",
    "\n",
    "    off_counts[label] = off_count\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "off_counts_df = pd.DataFrame.from_dict(off_counts, orient='index', columns=['Sensor OFF Count'])\n",
    "\n",
    "# Display results\n",
    "off_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique days where depletion occurred\n",
    "depleted_days = df_post_depletion['Timestamp'].dt.date.unique()\n",
    "\n",
    "# Loop through each depleted day for visualization\n",
    "for day in depleted_days:\n",
    "    # Filter data for the current depleted day\n",
    "    df_day_pre = df_pre_coverage[df_pre_coverage['Date'] == day]\n",
    "    df_day_new = df_new_coverage_only[df_new_coverage_only['Date'] == day]\n",
    "    df_day_actual = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "    df_day_predicted = df_post_depletion[df_post_depletion['Timestamp'].dt.date == day]\n",
    "\n",
    "    # Convert to GeoDataFrames\n",
    "    gdf_pre = grid_gdf.loc[grid_gdf.index.isin(df_day_pre['Grid_Cell'])].copy()\n",
    "    gdf_pre['Color'] = 'black'  # Pre-depletion trajectory\n",
    "\n",
    "    gdf_new = grid_gdf.loc[grid_gdf.index.isin(df_day_new['Grid_Cell'])].copy()\n",
    "    gdf_new['Color'] = 'green'  # Newly sensed due to 10min rule\n",
    "\n",
    "    gdf_actual = grid_gdf.loc[grid_gdf.index.isin(df_day_actual['Grid_Cell'])].copy()\n",
    "    gdf_actual['Color'] = 'red'  # Actual trajectory after depletion\n",
    "\n",
    "    predicted_grid_cells = df_day_predicted['Predicted_Grid_Cell'].dropna().unique()\n",
    "    gdf_predicted = grid_gdf.loc[grid_gdf.index.isin(predicted_grid_cells)].copy()\n",
    "    gdf_predicted['Color'] = 'blue'  # Predicted trajectory\n",
    "\n",
    "    # Skip if no relevant data for the day\n",
    "    if gdf_pre.empty and gdf_new.empty and gdf_actual.empty and gdf_predicted.empty:\n",
    "        print(f\"Skipping {day}: No valid data for plotting.\")\n",
    "        continue\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Plot Grid Cells (Background)\n",
    "    grid_gdf.plot(ax=ax, color='lightgrey', edgecolor='grey', alpha=0.2)\n",
    "\n",
    "    # Plot Pre-Depletion Trajectory (Black)\n",
    "    if not gdf_pre.empty:\n",
    "        gdf_pre.plot(ax=ax, color='black', alpha=0.5, edgecolor='black', label=\"Pre-Depletion Trajectory\")\n",
    "\n",
    "    # Plot Actual Post-Depletion Trajectory (Red)\n",
    "    if not gdf_actual.empty:\n",
    "        gdf_actual.plot(ax=ax, color='red', alpha=0.5, edgecolor='red', label=\"Actual Trajectory (Post-Depletion)\")\n",
    "\n",
    "    # Plot Predicted Post-Depletion Trajectory (Blue)\n",
    "    if not gdf_predicted.empty:\n",
    "        gdf_predicted.plot(ax=ax, color='blue', alpha=0.5, edgecolor='blue', label=\"Predicted Trajectory\")\n",
    "\n",
    "    # Plot Newly Sensed Cells Due to 10-Minute Rule (Green)\n",
    "    if not gdf_new.empty:\n",
    "        gdf_new.plot(ax=ax, color='green', alpha=0.5, edgecolor='green', label=\"Newly Sensed Cells (10-min Interval)\")\n",
    "\n",
    "    # Add Basemap\n",
    "    try:\n",
    "        ctx.add_basemap(ax, crs=grid_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    except Exception as e:\n",
    "        print(f\"Basemap Error on {day}: {e}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Trajectory Visualization with 10-Minute Sensing Constraint - {day}\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the bounds from the data\n",
    "min_lat, max_lat = df['Lat'].min(), df['Lat'].max()\n",
    "min_lon, max_lon = df['Log'].min(), df['Log'].max()\n",
    "\n",
    "# Define grid size (120x120 meters)\n",
    "grid_size = 120\n",
    "lat_resolution = grid_size / 111320  # Convert meters to latitude degrees\n",
    "lon_resolution_at_lat = lambda lat: grid_size / (111320 * np.cos(np.radians(lat)))\n",
    "\n",
    "# Generate grid covering the dataset area\n",
    "grid = []\n",
    "lat = min_lat\n",
    "while lat < max_lat:\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lon_res = lon_resolution_at_lat(lat)\n",
    "        grid.append(Polygon([\n",
    "            (lon, lat),\n",
    "            (lon + lon_res, lat),\n",
    "            (lon + lon_res, lat + lat_resolution),\n",
    "            (lon, lat + lat_resolution)\n",
    "        ]))\n",
    "        lon += lon_res\n",
    "    lat += lat_resolution\n",
    "\n",
    "# Create an empty GeoDataFrame for the grid\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': grid, 'Count': 0}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the data points\n",
    "df_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['Log'], df['Lat']), crs=\"EPSG:4326\")\n",
    "\n",
    "# Assign each measurement to a grid square\n",
    "for index, point in df_gdf.iterrows():\n",
    "    match = grid_gdf.contains(point.geometry)\n",
    "    if match.any():\n",
    "        grid_gdf.loc[match.idxmax(), 'Count'] += 1\n",
    "\n",
    "# Apply Fractional Power Scaling\n",
    "gamma = 0.3  # Adjust for visibility\n",
    "grid_gdf['Scaled_Count'] = (grid_gdf['Count'] + 1) ** gamma\n",
    "\n",
    "# Normalize values for color mapping\n",
    "norm = Normalize(vmin=grid_gdf['Scaled_Count'].min(), vmax=grid_gdf['Scaled_Count'].max())\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "# Convert scaled values to hex colors\n",
    "grid_gdf['Color'] = grid_gdf['Scaled_Count'].apply(lambda x: to_hex(cmap(norm(x))))\n",
    "\n",
    "# Create Folium map centered on Stockholm\n",
    "m = folium.Map(location=[df['Lat'].mean(), df['Log'].mean()], zoom_start=12, tiles='Cartodb dark_matter')\n",
    "\n",
    "# Function to color the grid based on scaled counts\n",
    "def style_function(feature):\n",
    "    color = feature['properties']['Color']  # Get precomputed color\n",
    "    return {\n",
    "        'fillColor': color,\n",
    "        'color': 'black',\n",
    "        'weight': 0.1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    "\n",
    "# Add grid layer to Folium\n",
    "folium.GeoJson(\n",
    "    grid_gdf,\n",
    "    name=\"Measurement Grid\",\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['Count'], aliases=[\"Measurements:\"])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Analysis\n",
    "#### Check Data Distribution\n",
    "- Before plotting, inspect the distribution of the Count column to confirm the skew. If the Count values have a large range (e.g., some counts are much higher than others), you can apply a logarithmic scale to the color mapping. This makes smaller variations more distinguishable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by spatial grid and count occurrences\n",
    "coverage = df.groupby(['Lat_Grid', 'Log_Grid']).size().reset_index(name='Count')\n",
    "\n",
    "# Count the frequency of each unique coverage count\n",
    "coverage_freq = coverage['Count'].value_counts().reset_index()\n",
    "coverage_freq.columns = ['Coverage Count', 'Frequency']\n",
    "\n",
    "# Sort in descending order\n",
    "coverage_freq = coverage_freq.sort_values(by='Coverage Count', ascending=False)\n",
    "\n",
    "# Find the maximum coverage count\n",
    "max_count = coverage['Count'].max()\n",
    "\n",
    "sns.histplot(coverage['Count'], bins=max_count, kde=True, color='blue')\n",
    "plt.title('Distribution of Coverage Counts')\n",
    "plt.xlabel('Coverage Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "# # Define Zipf-Mandelbrot function\n",
    "# def zipf_mandelbrot_func(r, s, q, C):\n",
    "#     return C / (r + np.abs(q)) ** s  # Ensure q is positive\n",
    "\n",
    "# # Define resolutions to test\n",
    "# spatial_resolutions = [0.00001, 0.0001, 0.001, 0.01]\n",
    "# temporal_resolutions = ['10S', '30S', '1T', '5T']\n",
    "\n",
    "# # Store results\n",
    "# results = []\n",
    "\n",
    "# for spatial_resolution in spatial_resolutions:\n",
    "#     for temporal_resolution in temporal_resolutions:\n",
    "#         # Create spatial grid\n",
    "#         df['Lat_Grid'] = (df['Lat'] // spatial_resolution) * spatial_resolution\n",
    "#         df['Log_Grid'] = (df['Log'] // spatial_resolution) * spatial_resolution\n",
    "        \n",
    "#         # Create temporal bins\n",
    "#         df['Time_Bin'] = df['Timestamp'].dt.floor(temporal_resolution)\n",
    "\n",
    "#         # Group by spatial grid and count occurrences\n",
    "#         coverage = df.groupby(['Lat_Grid', 'Log_Grid']).size().reset_index(name='Count')\n",
    "\n",
    "#         # Sort data in Zipfian order\n",
    "#         sorted_counts = np.sort(coverage['Count'])[::-1]  # Descending order\n",
    "#         ranks = np.arange(1, len(sorted_counts) + 1)  # Rank numbers\n",
    "\n",
    "#         # Fit Zipf-Mandelbrot\n",
    "#         try:\n",
    "#             params, _ = curve_fit(zipf_mandelbrot_func, ranks, sorted_counts, \n",
    "#                                   p0=[1, 1, max(sorted_counts)], \n",
    "#                                   bounds=([0.5, 0.0001, 0], [3, 10, np.inf]))\n",
    "\n",
    "#             s_fit, q_fit, C_fit = params\n",
    "#             expected_values = zipf_mandelbrot_func(ranks, s_fit, q_fit, C_fit)\n",
    "            \n",
    "#             # Compute residuals\n",
    "#             residuals = sorted_counts - expected_values\n",
    "#             std_residuals = np.std(residuals)\n",
    "            \n",
    "#             # Perform KS test\n",
    "#             ks_stat, p_value = kstest(sorted_counts, zipf_mandelbrot_func, args=(s_fit, q_fit, C_fit))\n",
    "\n",
    "#             # Compute AIC (Akaike Information Criterion)\n",
    "#             AIC = -2 * np.log(p_value) + 2 * 3  # 3 parameters: s, q, C\n",
    "\n",
    "#             # Store results\n",
    "#             results.append({\n",
    "#                 'Spatial_Resolution': spatial_resolution,\n",
    "#                 'Temporal_Resolution': temporal_resolution,\n",
    "#                 'KS_Statistic': ks_stat,\n",
    "#                 'p_value': p_value,\n",
    "#                 'Std_Residuals': std_residuals,\n",
    "#                 'AIC': AIC\n",
    "#             })\n",
    "\n",
    "#         except RuntimeError:\n",
    "#             print(f\"Fit failed for Spatial={spatial_resolution}, Temporal={temporal_resolution}\")\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # Select the best resolution (min AIC, high p-value, low KS statistic)\n",
    "# best_result = results_df.sort_values(by=['AIC', 'KS_Statistic'], ascending=[True, True]).iloc[0]\n",
    "# print(\"Best Resolution Parameters:\")\n",
    "# print(best_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort coverage counts in descending order (ranked frequencies)\n",
    "sorted_counts = np.sort(coverage['Count'])[::-1]  # Descending order\n",
    "ranks = np.arange(1, len(sorted_counts) + 1)  # Rank numbers\n",
    "\n",
    "# Define Zipf-Mandelbrot function: f(r) = C / (r + q)^s\n",
    "def zipf_mandelbrot_func(r, s, q, C):\n",
    "    return C / (r + np.abs(q)) ** s  # Ensure q is positive\n",
    "\n",
    "# Fit Zipf-Mandelbrot with constraints to avoid numerical issues\n",
    "params, _ = curve_fit(zipf_mandelbrot_func, ranks, sorted_counts, p0=[1, 1, max(sorted_counts)], bounds=([0.5, 0.0001, 0], [3, 10, np.inf]))\n",
    "s_fit, q_fit, C_fit = params\n",
    "\n",
    "# Compute expected values from the fitted Zipf-Mandelbrot model\n",
    "expected_values = zipf_mandelbrot_func(ranks, s_fit, q_fit, C_fit)\n",
    "\n",
    "# Compute residuals (Observed - Expected)\n",
    "residuals = sorted_counts - expected_values\n",
    "relative_residuals = residuals / expected_values  # Normalize residuals\n",
    "\n",
    "# Plot Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(ranks, residuals, alpha=0.6, color=\"red\", label=\"Residuals (Observed - Expected)\")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", alpha=0.6)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Residual (Observed - Expected)\", fontsize=12)\n",
    "plt.title(\"Residuals from Zipf-Mandelbrot Fit\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for outliers (e.g., 1.2x expected value)\n",
    "tolerance_factor = 3\n",
    "\n",
    "# Identify outliers (values too far from expected Zipfian behavior)\n",
    "outlier_mask = (sorted_counts > expected_values * tolerance_factor) | (sorted_counts < expected_values / tolerance_factor)\n",
    "\n",
    "# Count the number of removed points\n",
    "num_outliers = outlier_mask.sum()\n",
    "print(f\"Number of detected outliers: {num_outliers}\")\n",
    "\n",
    "# Remove outliers from dataset\n",
    "filtered_counts = sorted_counts[~outlier_mask]\n",
    "filtered_ranks = ranks[~outlier_mask]\n",
    "\n",
    "# Exclude extreme values (top 5% and bottom 5%)\n",
    "lower_bound_c = int(0.1 * len(filtered_counts))\n",
    "upper_bound_c = int(0.9 * len(filtered_counts))\n",
    "filtered_counts=filtered_counts[lower_bound_c:upper_bound_c]\n",
    "lower_bound_r = int(0.1 * len(filtered_ranks))\n",
    "upper_bound_r = int(0.9 * len(filtered_ranks))\n",
    "filtered_ranks = filtered_ranks[lower_bound_r:upper_bound_r]\n",
    "\n",
    "# Re-Fit Zipf-Mandelbrot with filtered data\n",
    "params_filtered, _ = curve_fit(\n",
    "    zipf_mandelbrot_func, \n",
    "    filtered_ranks, \n",
    "    filtered_counts, \n",
    "    p0=[1, 1, max(filtered_counts)], \n",
    "    bounds=([0.5, 0.0001, 0], [3, 10, np.inf])\n",
    ")\n",
    "\n",
    "# Extract new parameters\n",
    "s_fit_filtered, q_fit_filtered, C_fit_filtered = params_filtered\n",
    "\n",
    "# Compute expected values with new parameters\n",
    "expected_values_filtered = zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    "\n",
    "# Plot cleaned data vs. new Zipf-Mandelbrot fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_ranks, filtered_counts, label=\"Filtered Data (No Outliers)\", alpha=0.6, color=\"blue\")\n",
    "plt.plot(filtered_ranks, expected_values_filtered, 'r-', linewidth=2, label=\"Re-Fitted Zipf-Mandelbrot Model\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Coverage Count\", fontsize=12)\n",
    "plt.title(\"Zipf-Mandelbrot Fit After Outlier Removal & Re-Fitting\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude extreme values (top 5% and bottom 5%)\n",
    "lower_bound = int(0.1 * len(filtered_counts))\n",
    "upper_bound = int(0.9 * len(filtered_counts))\n",
    "\n",
    "ks_stat_truncated, p_value_truncated = kstest(\n",
    "    filtered_counts[lower_bound:upper_bound], \n",
    "    zipf_mandelbrot_func, \n",
    "    args=(s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    ")\n",
    "ks_stat_after, p_value_after = kstest(filtered_counts, zipf_mandelbrot_func, args=(s_fit_filtered, q_fit_filtered, C_fit_filtered))\n",
    "\n",
    "\n",
    "print(f\"Truncated KS Test - Statistic: {ks_stat_truncated:.4f}, p-value: {p_value_truncated:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estimated Zipf-Mandelbrot Exponent (s): {s_fit:.4f}\")\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute log-likelihood\n",
    "log_likelihood = np.sum(norm.logpdf(filtered_counts, loc=zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered), scale=np.std(filtered_counts)))\n",
    "AIC_fixed = -2 * log_likelihood + 2 * 3  # 3 parameters: s, q, C\n",
    "\n",
    "print(f\"Fixed AIC: {AIC_fixed:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pure Zipf function: f(r) = C / r^s\n",
    "def zipf_func(r, s, C):\n",
    "    return C / r ** s\n",
    "\n",
    "# Fit pure Zipf\n",
    "params_zipf, _ = curve_fit(zipf_func, filtered_ranks, filtered_counts, \n",
    "                           p0=[1, max(filtered_counts)], \n",
    "                           bounds=([0.5, 0], [3, np.inf]))\n",
    "\n",
    "s_zipf, C_zipf = params_zipf\n",
    "expected_values_zipf = zipf_func(filtered_ranks, s_zipf, C_zipf)\n",
    "\n",
    "# Compute Log-Likelihood and AIC for pure Zipf\n",
    "log_likelihood_zipf = np.sum(norm.logpdf(\n",
    "    filtered_counts, \n",
    "    loc=expected_values_zipf, \n",
    "    scale=np.std(filtered_counts)\n",
    "))\n",
    "AIC_zipf = -2 * log_likelihood_zipf + 2 * 2  # 2 parameters: s, C\n",
    "\n",
    "print(f\"Pure Zipf Log-Likelihood: {log_likelihood_zipf:.4f}\")\n",
    "print(f\"Pure Zipf AIC: {AIC_zipf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10% (frequent ranks)\n",
    "top_residuals = filtered_counts[:int(0.1 * len(filtered_counts))] - expected_values_filtered[:int(0.1 * len(filtered_counts))]\n",
    "\n",
    "# Bottom 10% (rare ranks)\n",
    "bottom_residuals = filtered_counts[-int(0.1 * len(filtered_counts)):] - expected_values_filtered[-int(0.1 * len(filtered_counts)):]\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(filtered_ranks[:int(0.1 * len(filtered_ranks))], top_residuals, color='red', alpha=0.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Top 10% Residuals\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(filtered_ranks[-int(0.1 * len(filtered_ranks)):], bottom_residuals, color='blue', alpha=0.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Bottom 10% Residuals\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on middle 80% of ranks\n",
    "lower_bound = int(0.1 * len(filtered_counts))\n",
    "upper_bound = int(0.9 * len(filtered_counts))\n",
    "\n",
    "ks_stat_truncated, p_value_truncated = kstest(\n",
    "    filtered_counts[lower_bound:upper_bound], \n",
    "    zipf_mandelbrot_func, \n",
    "    args=(s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    ")\n",
    "\n",
    "print(f\"Truncated KS Test - Statistic: {ks_stat_truncated:.4f}, p-value: {p_value_truncated:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_values_filtered = zipf_mandelbrot_func(filtered_ranks, s_fit_filtered, q_fit_filtered, C_fit_filtered)\n",
    "residuals_filtered = filtered_counts - expected_values_filtered\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_ranks, residuals_filtered, alpha=0.6, color=\"red\", label=\"Residuals (Observed - Expected)\")\n",
    "\n",
    "plt.axhline(0, linestyle=\"--\", color=\"black\", alpha=0.6)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"linear\")\n",
    "plt.xlabel(\"Rank\", fontsize=12)\n",
    "plt.ylabel(\"Residual (Observed - Expected)\", fontsize=12)\n",
    "plt.title(\"Residuals After Outlier Removal\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "- Analyze sensor coverage by aggregating the spatial grid.\n",
    "- Visualize coverage heatmaps. To better visualize the data, apply logarithmic scaling to the color values. This will compress the range of large values and expand the smaller values for more differentiation in color. \n",
    "- We apply Fractional Power Scaling: Highlights smaller values significantly, making subtle differences more visible. We Raise the log-transformed values to a fractional power $ \\log(x+1)^{0.5} $. This amplifies small differences while keeping the general scale.\n",
    "##### Note:\n",
    "- If \\( x = 0 \\), the standard `np.log(x)` would result in an error because the logarithm of 0 is undefined. \n",
    "`np.log1p(x)` handles this safely by adding \\( 1 \\) to the input before computing the logarithm, ensuring it works for non-negative numbers, including \\( 0 \\).\n",
    "- The square root further compresses the range of the values.  \n",
    "It emphasizes smaller differences by reducing the impact of large values. For example:  \n",
    "$\\log(x+1)^{0.5} $ grows slower than $\\log(x+1)$ as $\\ x $ increases.\n",
    "\n",
    "**This transformation is particularly useful for skewed data, for `Count` values, where:**\n",
    "\n",
    "- Most data points are small.\n",
    "- A few extreme values (outliers) dominate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Apply logarithmic scaling\n",
    "log_scaled_values = np.log1p(coverage['Count'])**0.5 \n",
    "\n",
    "# Apply logarithmic scaling to color values\n",
    "sc = plt.scatter(\n",
    "    coverage['Log_Grid'], \n",
    "    coverage['Lat_Grid'], \n",
    "    c=log_scaled_values,  \n",
    "    cmap='jet', \n",
    "    s=30, \n",
    "    edgecolor='k', \n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add a color bar with the original scale in the label\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('âˆš(Log(Coverage Count + 1))', fontsize=12)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Add labels and title with improved font sizes\n",
    "plt.xlabel('Longitude Grid', fontsize=14)\n",
    "plt.ylabel('Latitude Grid', fontsize=14)\n",
    "plt.title('Coverage Heatmap with Logarithmic Scale', fontsize=16)\n",
    "\n",
    "# Add grid lines for reference\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Improve tick sizes for better readability\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Temporal Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time difference in seconds between consecutive rows\n",
    "df['Delta_t'] = df['Timestamp'].diff().dt.total_seconds()  \n",
    "\n",
    "# Define an expected interval in seconds (e.g., 60 seconds)\n",
    "expected_interval = 60\n",
    "\n",
    "# Count the total number of occurrences of measurements\n",
    "tot_count = df['Delta_t'].count()\n",
    "print(f\"Number of values: {tot_count}\")\n",
    "\n",
    "# Count the number of occurrences of low frequency measurements\n",
    "highf_count = (df['Delta_t'] > expected_interval).sum()\n",
    "print(f\"Number of values higher than 60sec: {highf_count}\")\n",
    "\n",
    "# Count the number of occurrences of 0.0\n",
    "zero_count = (df['Timestamp'].diff().dt.total_seconds() == 0.0).sum()\n",
    "print(f\"Number of values equal to 0.0sec: {zero_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy and Coverage Model Preparation\n",
    "- Create columns to represent:\n",
    "\n",
    "    - Whether a street segment is already covered.\n",
    "    - Battery state changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering out Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time difference in seconds between consecutive rows\n",
    "df['Delta_t'] = df['Timestamp'].diff().dt.total_seconds()  \n",
    "\n",
    "# Define a threshold for acceptable intervals (e.g., 60 seconds)\n",
    "acceptable_threshold = 60   # in seconds\n",
    "\n",
    "# Filter out rows with large Delta_t\n",
    "df = df[df['Delta_t'] <= acceptable_threshold]\n",
    "\n",
    "# Drop rows with Delta_t equal to zero\n",
    "df = df[df['Delta_t'] > 0]\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The battery capacity is 10,000 mAh, so SOC_Change (calculated from current and time) must be converted into a percentage of the total capacity before being added to `SOC_batt`.\n",
    "\n",
    "### SOC Update Formula:\n",
    "$SOC_{new} = SOC_{old} + \\frac{\\Delta SOC_{mAh}}{C_{batt}} \\times 100$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\Delta SOC_{mAh} = -1 \\times I_{batt} \\times \\Delta t$ (mAh change based on current and time)\n",
    "\n",
    "Where:\n",
    "- $SOC_{old}$ is $SOC_{batt}$\n",
    "- $\\ I_{batt} $: Net current (`current_batt`) in mA (positive for consumption, negative for storage).\n",
    "- $\\ \\Delta t $: Time difference in hours between consecutive rows.\n",
    "- $\\ C_{batt} $: Battery capacity in mAh (10,000 mAh).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Battery capacity in mAh\n",
    "battery_capacity = 10000\n",
    "\n",
    "# Calculate time difference in hours between consecutive rows\n",
    "df['Delta_t'] = df['Delta_t'] / 3600  # Time difference in hours\n",
    "\n",
    "# Calculate SOC change (in %) using the corrected formula\n",
    "df['SOC_Change'] = (-1 * df['current_batt'] * df['Delta_t'] / battery_capacity) * 100\n",
    "\n",
    "# Set SOC_Change to 0 when SOC is saturated\n",
    "df.loc[df['SOC_batt'] >= 90, 'SOC_Change'] = 0\n",
    "\n",
    "# Ensure SOC values are capped between 0 and 100\n",
    "df['SOC_batt'] = df['SOC_batt'] + df['SOC_Change']\n",
    "df['SOC_batt'] = df['SOC_batt'].clip(lower=0, upper=100)\n",
    "\n",
    "# Assume that is >90%, charging is stopped\n",
    "df.loc[df['SOC_batt'] >= 90, 'SOC_Change'] = 0\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(df[['Timestamp', 'Lat', 'Log', 'SOC_batt', 'SOC_Change', 'Delta_t', 'current_batt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SOC over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Timestamp'], df['SOC_batt'], label='State of Charge (SOC)', color='blue', linewidth=1.5)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('SOC (%)', fontsize=14)\n",
    "plt.title('Battery State of Charge Over Time', fontsize=16)\n",
    "plt.grid(alpha=0.5, linestyle='--')\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where SOC is increasing or decreasing\n",
    "increasing = df[df['SOC_Change'] > 0]\n",
    "decreasing = df[df['SOC_Change'] < 0]\n",
    "\n",
    "print(f\"Number of times SOC increases: {len(increasing)}\")\n",
    "print(f\"Number of times SOC decreases: {len(decreasing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark covered segments (spatio-temporal condition)\n",
    "df['Is_Covered'] = df.duplicated(subset=['Lat_Grid', 'Log_Grid', 'Time_Bin'], keep='first')\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_aware_switch(row):\n",
    "    if row['Is_Covered'] and row['SOC_batt'] > 20:\n",
    "        return 'OFF'\n",
    "    elif not row['Is_Covered'] and row['SOC_batt'] > 10:\n",
    "        return 'ON'\n",
    "    else:\n",
    "        return 'IDLE'\n",
    "\n",
    "df['Sensor_State'] = df.apply(energy_aware_switch, axis=1)\n",
    "\n",
    "df['Cumulative_Spatial_Coverage'] = (~df['Is_Covered']).cumsum()\n",
    "temporal_coverage = df.groupby(['Lat_Grid', 'Log_Grid', 'Time_Bin']).size().reset_index(name='Frequency')\n",
    "\n",
    "high_coverage_cells = temporal_coverage[temporal_coverage['Frequency'] > 2]\n",
    "high_coverage_cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cumulative coverage and energy tracking\n",
    "df['Cumulative_Coverage'] = 0\n",
    "df['Average_SOC'] = 0\n",
    "\n",
    "# Initialize variables\n",
    "cumulative_coverage = set()  # To store unique covered grid cells\n",
    "soc_list = []  # To store SOC levels\n",
    "\n",
    "# Simulate over the data\n",
    "for idx, row in df.iterrows():\n",
    "    # Update cumulative coverage if sensor is ON\n",
    "    if row['Sensor_State'] == 'ON':\n",
    "        cumulative_coverage.add((row['Lat_Grid'], row['Log_Grid']))\n",
    "\n",
    "    # Update SOC tracking\n",
    "    soc_list.append(row['SOC_batt'])\n",
    "\n",
    "    # Update DataFrame\n",
    "    df.at[idx, 'Cumulative_Coverage'] = len(cumulative_coverage)\n",
    "    df.at[idx, 'Average_SOC'] = sum(soc_list) / len(soc_list)\n",
    "\n",
    "# Preview results\n",
    "print(df[['Timestamp', 'Cumulative_Coverage', 'Average_SOC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative coverage over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Timestamp'], df['Cumulative_Coverage'], label='Cumulative Coverage', color='b')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Cumulative Coverage', fontsize=14)\n",
    "plt.title('Cumulative Coverage Over Time', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot average SOC over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Timestamp'], df['Average_SOC'], label='Average SOC', color='g')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Average SOC (%)', fontsize=14)\n",
    "plt.title('Average SOC Over Time', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: All sensors ON\n",
    "df['Baseline_Coverage'] = 0\n",
    "df['Baseline_SOC'] = 0.0\n",
    "\n",
    "# Initialize variables\n",
    "baseline_coverage = set()\n",
    "baseline_soc_list = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Assume sensors are always ON\n",
    "    baseline_coverage.add((row['Lat_Grid'], row['Log_Grid']))\n",
    "    baseline_soc_list.append(row['SOC_batt'])\n",
    "\n",
    "    # Update DataFrame\n",
    "    df.at[idx, 'Baseline_Coverage'] = len(baseline_coverage)\n",
    "    df.at[idx, 'Baseline_SOC'] = sum(baseline_soc_list) / len(baseline_soc_list)\n",
    "\n",
    "# Compare cumulative coverage and SOC\n",
    "print(df[['Timestamp', 'Cumulative_Coverage', 'Baseline_Coverage', 'Average_SOC', 'Baseline_SOC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cumulative coverage\n",
    "energy_aware_coverage = df['Cumulative_Coverage'].iloc[-1]\n",
    "baseline_coverage = df['Baseline_Coverage'].iloc[-1]\n",
    "\n",
    "# Average SOC\n",
    "energy_aware_avg_soc = df['Average_SOC'].mean()\n",
    "baseline_avg_soc = df['Baseline_SOC'].mean()\n",
    "\n",
    "# Improvement metrics\n",
    "coverage_improvement = (energy_aware_coverage - baseline_coverage) / baseline_coverage * 100\n",
    "soc_savings = (baseline_avg_soc - energy_aware_avg_soc) / baseline_avg_soc * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Energy-Aware Total Coverage: {energy_aware_coverage}\")\n",
    "print(f\"Baseline Total Coverage: {baseline_coverage}\")\n",
    "print(f\"Coverage Improvement: {coverage_improvement:.2f}%\")\n",
    "print(f\"Energy Savings: {soc_savings:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scl_rf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
